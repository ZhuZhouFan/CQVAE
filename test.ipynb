{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from DGP import DGP_gu, DGP_t3\n",
    "from AE import AE_Agent, AE_Factor_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_dic = {'zero':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5, 'six':6, 'seven':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda setting\n",
    "torch.cuda.set_device(3)\n",
    "# DGP parameters\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000\n",
    "T = 400\n",
    "P_x = P_c = 50\n",
    "P_f = 3\n",
    "W = np.hstack([np.identity(P_f), np.zeros([P_f, P_x - P_f])])\n",
    "linear_index = True\n",
    "# network parameters\n",
    "f_hidden_dim = 64\n",
    "K = 1\n",
    "lr = 1e-3\n",
    "lam = 1e-4\n",
    "AE_epoch_num = 5000\n",
    "AE_factor_epoch_num = 5000\n",
    "# the number of repetition and log_dir\n",
    "repetition_num = 3\n",
    "bandwidth = 10\n",
    "log_name = 'test'\n",
    "time_ = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "root_log_dir = f'{log_name}_{seed}_{time_}'\n",
    "if not os.path.exists(root_log_dir):\n",
    "    os.makedirs(root_log_dir)\n",
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, C = DGP_t3(N, T, P_f, P_x, P_c, W, linear_index)\n",
    "repetition_index = 0\n",
    "P = C.shape[2]\n",
    "log_dir = f'{root_log_dir}/{repetition_index}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = np.zeros((P, T))\n",
    "for t in range(T):\n",
    "    portfolio[:, t] = np.linalg.inv(C[:, t, :].transpose() @ C[:, t, :]) @ C[:, t, :].transpose() @ r[:, t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_agent = AE_Agent(input_dim = P,\n",
    "                    latent_dim = K,\n",
    "                    output_dim = P,\n",
    "                    learning_rate = lr,\n",
    "                    seed = seed + repetition_index,\n",
    "                    log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining loading...\n",
      "The data contains 133 training samples and 134 valid samples\n",
      "Complete!\n",
      "best score:0.6589850783348083\n"
     ]
    }
   ],
   "source": [
    "AE_feature = AE_label = torch.Tensor(portfolio.transpose())\n",
    "AE_agent.load_data(feature = AE_feature, label = AE_label, valid_size = 1/3, test_size = 1/3, num_cpu = 0, batch_size = 64)\n",
    "AE_agent.train(AE_epoch_num, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_model_para = torch.load(f'{log_dir}/AE_best.pth')\n",
    "AE_factor = AE_Factor_Agent(N = N,\n",
    "                            T = T,\n",
    "                            P = P,\n",
    "                            K = K,\n",
    "                            f_hidden_dim = f_hidden_dim,\n",
    "                            model_para = AE_model_para,\n",
    "                            learning_rate = lr, \n",
    "                            seed = seed + repetition_index,\n",
    "                            log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining loading...\n",
      "The data contains 133 training samples, 134 validation samples and 133 test samples\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "AE_factor.load_data(C = C, r = r, valid_size = 1/3, test_size =1/3, batch_size = 64, num_cpu = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4999 Training Loss 1794.08 Time Consume 2.231\n",
      "------------------------------------------------------------\n",
      "Evaluation 0/4999 Loss 1752.40\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1/4999 Training Loss 1736.00 Time Consume 1.913\n",
      "Epoch 2/4999 Training Loss 1696.19 Time Consume 1.902\n",
      "Epoch 3/4999 Training Loss 1670.15 Time Consume 1.858\n",
      "Epoch 4/4999 Training Loss 1652.98 Time Consume 1.842\n",
      "Epoch 5/4999 Training Loss 1640.13 Time Consume 1.896\n",
      "Epoch 6/4999 Training Loss 1629.39 Time Consume 1.900\n",
      "Epoch 7/4999 Training Loss 1618.62 Time Consume 1.887\n",
      "Epoch 8/4999 Training Loss 1607.63 Time Consume 1.907\n",
      "Epoch 9/4999 Training Loss 1598.53 Time Consume 1.884\n",
      "Epoch 10/4999 Training Loss 1588.70 Time Consume 1.897\n",
      "------------------------------------------------------------\n",
      "Evaluation 10/4999 Loss 1567.59\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 11/4999 Training Loss 1578.80 Time Consume 1.909\n",
      "Epoch 12/4999 Training Loss 1569.75 Time Consume 1.959\n",
      "Epoch 13/4999 Training Loss 1560.30 Time Consume 1.894\n",
      "Epoch 14/4999 Training Loss 1551.31 Time Consume 1.880\n",
      "Epoch 15/4999 Training Loss 1542.36 Time Consume 1.793\n",
      "Epoch 16/4999 Training Loss 1532.02 Time Consume 1.852\n",
      "Epoch 17/4999 Training Loss 1521.85 Time Consume 1.917\n",
      "Epoch 18/4999 Training Loss 1509.75 Time Consume 1.841\n",
      "Epoch 19/4999 Training Loss 1499.02 Time Consume 1.869\n",
      "Epoch 20/4999 Training Loss 1486.31 Time Consume 1.854\n",
      "------------------------------------------------------------\n",
      "Evaluation 20/4999 Loss 1454.14\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 21/4999 Training Loss 1475.07 Time Consume 1.872\n",
      "Epoch 22/4999 Training Loss 1461.62 Time Consume 1.903\n",
      "Epoch 23/4999 Training Loss 1447.12 Time Consume 1.737\n",
      "Epoch 24/4999 Training Loss 1431.59 Time Consume 1.976\n",
      "Epoch 25/4999 Training Loss 1418.14 Time Consume 2.005\n",
      "Epoch 26/4999 Training Loss 1402.24 Time Consume 1.887\n",
      "Epoch 27/4999 Training Loss 1386.72 Time Consume 1.913\n",
      "Epoch 28/4999 Training Loss 1372.57 Time Consume 1.864\n",
      "Epoch 29/4999 Training Loss 1356.20 Time Consume 1.798\n",
      "Epoch 30/4999 Training Loss 1342.38 Time Consume 1.954\n",
      "------------------------------------------------------------\n",
      "Evaluation 30/4999 Loss 1302.23\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 31/4999 Training Loss 1328.25 Time Consume 1.878\n",
      "Epoch 32/4999 Training Loss 1315.54 Time Consume 1.880\n",
      "Epoch 33/4999 Training Loss 1303.48 Time Consume 1.896\n",
      "Epoch 34/4999 Training Loss 1292.02 Time Consume 1.898\n",
      "Epoch 35/4999 Training Loss 1282.10 Time Consume 1.886\n",
      "Epoch 36/4999 Training Loss 1274.07 Time Consume 1.739\n",
      "Epoch 37/4999 Training Loss 1266.74 Time Consume 1.744\n",
      "Epoch 38/4999 Training Loss 1260.47 Time Consume 1.862\n",
      "Epoch 39/4999 Training Loss 1254.43 Time Consume 1.905\n",
      "Epoch 40/4999 Training Loss 1249.20 Time Consume 1.877\n",
      "------------------------------------------------------------\n",
      "Evaluation 40/4999 Loss 1230.48\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 41/4999 Training Loss 1244.41 Time Consume 1.907\n",
      "Epoch 42/4999 Training Loss 1240.50 Time Consume 1.877\n",
      "Epoch 43/4999 Training Loss 1236.86 Time Consume 1.881\n",
      "Epoch 44/4999 Training Loss 1233.39 Time Consume 1.837\n",
      "Epoch 45/4999 Training Loss 1230.34 Time Consume 1.818\n",
      "Epoch 46/4999 Training Loss 1227.62 Time Consume 1.745\n",
      "Epoch 47/4999 Training Loss 1224.84 Time Consume 1.775\n",
      "Epoch 48/4999 Training Loss 1222.66 Time Consume 1.757\n",
      "Epoch 49/4999 Training Loss 1220.27 Time Consume 1.756\n",
      "Epoch 50/4999 Training Loss 1218.28 Time Consume 1.797\n",
      "------------------------------------------------------------\n",
      "Evaluation 50/4999 Loss 1208.50\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 51/4999 Training Loss 1216.39 Time Consume 1.864\n",
      "Epoch 52/4999 Training Loss 1214.59 Time Consume 1.927\n",
      "Epoch 53/4999 Training Loss 1212.90 Time Consume 1.941\n",
      "Epoch 54/4999 Training Loss 1211.35 Time Consume 2.081\n",
      "Epoch 55/4999 Training Loss 1209.88 Time Consume 1.878\n",
      "Epoch 56/4999 Training Loss 1208.29 Time Consume 1.867\n",
      "Epoch 57/4999 Training Loss 1207.00 Time Consume 1.935\n",
      "Epoch 58/4999 Training Loss 1205.45 Time Consume 1.915\n",
      "Epoch 59/4999 Training Loss 1204.15 Time Consume 1.959\n",
      "Epoch 60/4999 Training Loss 1202.85 Time Consume 1.754\n",
      "------------------------------------------------------------\n",
      "Evaluation 60/4999 Loss 1195.38\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 61/4999 Training Loss 1201.51 Time Consume 1.911\n",
      "Epoch 62/4999 Training Loss 1200.23 Time Consume 1.917\n",
      "Epoch 63/4999 Training Loss 1198.95 Time Consume 1.907\n",
      "Epoch 64/4999 Training Loss 1197.62 Time Consume 1.866\n",
      "Epoch 65/4999 Training Loss 1196.42 Time Consume 1.912\n",
      "Epoch 66/4999 Training Loss 1195.23 Time Consume 1.860\n",
      "Epoch 67/4999 Training Loss 1193.99 Time Consume 1.890\n",
      "Epoch 68/4999 Training Loss 1192.73 Time Consume 1.875\n",
      "Epoch 69/4999 Training Loss 1191.53 Time Consume 1.881\n",
      "Epoch 70/4999 Training Loss 1190.39 Time Consume 1.790\n",
      "------------------------------------------------------------\n",
      "Evaluation 70/4999 Loss 1183.18\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 71/4999 Training Loss 1189.24 Time Consume 1.924\n",
      "Epoch 72/4999 Training Loss 1187.99 Time Consume 1.776\n",
      "Epoch 73/4999 Training Loss 1186.85 Time Consume 1.842\n",
      "Epoch 74/4999 Training Loss 1185.78 Time Consume 1.815\n",
      "Epoch 75/4999 Training Loss 1184.60 Time Consume 1.868\n",
      "Epoch 76/4999 Training Loss 1183.39 Time Consume 1.895\n",
      "Epoch 77/4999 Training Loss 1182.23 Time Consume 1.899\n",
      "Epoch 78/4999 Training Loss 1181.13 Time Consume 1.899\n",
      "Epoch 79/4999 Training Loss 1179.89 Time Consume 1.953\n",
      "Epoch 80/4999 Training Loss 1178.77 Time Consume 1.890\n",
      "------------------------------------------------------------\n",
      "Evaluation 80/4999 Loss 1171.09\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 81/4999 Training Loss 1177.55 Time Consume 1.854\n",
      "Epoch 82/4999 Training Loss 1176.43 Time Consume 1.876\n",
      "Epoch 83/4999 Training Loss 1175.27 Time Consume 1.900\n",
      "Epoch 84/4999 Training Loss 1174.09 Time Consume 1.810\n",
      "Epoch 85/4999 Training Loss 1173.03 Time Consume 1.870\n",
      "Epoch 86/4999 Training Loss 1171.84 Time Consume 1.963\n",
      "Epoch 87/4999 Training Loss 1170.70 Time Consume 1.945\n",
      "Epoch 88/4999 Training Loss 1169.56 Time Consume 1.931\n",
      "Epoch 89/4999 Training Loss 1168.37 Time Consume 1.805\n",
      "Epoch 90/4999 Training Loss 1167.23 Time Consume 1.878\n",
      "------------------------------------------------------------\n",
      "Evaluation 90/4999 Loss 1159.20\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 91/4999 Training Loss 1166.10 Time Consume 1.856\n",
      "Epoch 92/4999 Training Loss 1164.89 Time Consume 1.858\n",
      "Epoch 93/4999 Training Loss 1163.73 Time Consume 1.879\n",
      "Epoch 94/4999 Training Loss 1162.77 Time Consume 1.848\n",
      "Epoch 95/4999 Training Loss 1161.50 Time Consume 1.870\n",
      "Epoch 96/4999 Training Loss 1160.39 Time Consume 1.883\n",
      "Epoch 97/4999 Training Loss 1159.34 Time Consume 1.884\n",
      "Epoch 98/4999 Training Loss 1158.11 Time Consume 1.936\n",
      "Epoch 99/4999 Training Loss 1157.03 Time Consume 1.928\n",
      "Epoch 100/4999 Training Loss 1155.89 Time Consume 1.909\n",
      "------------------------------------------------------------\n",
      "Evaluation 100/4999 Loss 1147.66\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 101/4999 Training Loss 1154.70 Time Consume 1.940\n",
      "Epoch 102/4999 Training Loss 1153.52 Time Consume 1.898\n",
      "Epoch 103/4999 Training Loss 1152.49 Time Consume 1.902\n",
      "Epoch 104/4999 Training Loss 1151.37 Time Consume 1.877\n",
      "Epoch 105/4999 Training Loss 1150.21 Time Consume 1.891\n",
      "Epoch 106/4999 Training Loss 1149.15 Time Consume 1.759\n",
      "Epoch 107/4999 Training Loss 1148.10 Time Consume 1.758\n",
      "Epoch 108/4999 Training Loss 1146.86 Time Consume 1.875\n",
      "Epoch 109/4999 Training Loss 1145.76 Time Consume 1.841\n",
      "Epoch 110/4999 Training Loss 1144.63 Time Consume 1.863\n",
      "------------------------------------------------------------\n",
      "Evaluation 110/4999 Loss 1136.31\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 111/4999 Training Loss 1143.55 Time Consume 1.944\n",
      "Epoch 112/4999 Training Loss 1142.44 Time Consume 1.936\n",
      "Epoch 113/4999 Training Loss 1141.36 Time Consume 1.976\n",
      "Epoch 114/4999 Training Loss 1140.20 Time Consume 1.901\n",
      "Epoch 115/4999 Training Loss 1139.09 Time Consume 1.934\n",
      "Epoch 116/4999 Training Loss 1137.99 Time Consume 1.933\n",
      "Epoch 117/4999 Training Loss 1136.85 Time Consume 1.934\n",
      "Epoch 118/4999 Training Loss 1135.83 Time Consume 1.838\n",
      "Epoch 119/4999 Training Loss 1134.65 Time Consume 1.871\n",
      "Epoch 120/4999 Training Loss 1133.49 Time Consume 1.913\n",
      "------------------------------------------------------------\n",
      "Evaluation 120/4999 Loss 1124.96\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 121/4999 Training Loss 1132.46 Time Consume 1.941\n",
      "Epoch 122/4999 Training Loss 1131.38 Time Consume 1.887\n",
      "Epoch 123/4999 Training Loss 1130.18 Time Consume 1.896\n",
      "Epoch 124/4999 Training Loss 1128.99 Time Consume 1.906\n",
      "Epoch 125/4999 Training Loss 1127.91 Time Consume 1.932\n",
      "Epoch 126/4999 Training Loss 1126.79 Time Consume 1.896\n",
      "Epoch 127/4999 Training Loss 1125.60 Time Consume 1.896\n",
      "Epoch 128/4999 Training Loss 1124.43 Time Consume 1.896\n",
      "Epoch 129/4999 Training Loss 1123.38 Time Consume 1.863\n",
      "Epoch 130/4999 Training Loss 1122.18 Time Consume 2.116\n",
      "------------------------------------------------------------\n",
      "Evaluation 130/4999 Loss 1113.53\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 131/4999 Training Loss 1121.06 Time Consume 1.888\n",
      "Epoch 132/4999 Training Loss 1119.87 Time Consume 1.871\n",
      "Epoch 133/4999 Training Loss 1118.71 Time Consume 1.874\n",
      "Epoch 134/4999 Training Loss 1117.53 Time Consume 1.947\n",
      "Epoch 135/4999 Training Loss 1116.42 Time Consume 1.940\n",
      "Epoch 136/4999 Training Loss 1115.26 Time Consume 1.961\n",
      "Epoch 137/4999 Training Loss 1114.06 Time Consume 1.878\n",
      "Epoch 138/4999 Training Loss 1112.99 Time Consume 1.894\n",
      "Epoch 139/4999 Training Loss 1111.77 Time Consume 2.004\n",
      "Epoch 140/4999 Training Loss 1110.68 Time Consume 1.892\n",
      "------------------------------------------------------------\n",
      "Evaluation 140/4999 Loss 1102.13\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 141/4999 Training Loss 1109.50 Time Consume 1.878\n",
      "Epoch 142/4999 Training Loss 1108.31 Time Consume 1.871\n",
      "Epoch 143/4999 Training Loss 1107.18 Time Consume 1.872\n",
      "Epoch 144/4999 Training Loss 1106.07 Time Consume 1.898\n",
      "Epoch 145/4999 Training Loss 1104.86 Time Consume 1.875\n",
      "Epoch 146/4999 Training Loss 1103.67 Time Consume 1.863\n",
      "Epoch 147/4999 Training Loss 1102.53 Time Consume 1.906\n",
      "Epoch 148/4999 Training Loss 1101.31 Time Consume 2.004\n",
      "Epoch 149/4999 Training Loss 1100.14 Time Consume 1.897\n",
      "Epoch 150/4999 Training Loss 1098.94 Time Consume 1.847\n",
      "------------------------------------------------------------\n",
      "Evaluation 150/4999 Loss 1090.46\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 151/4999 Training Loss 1097.81 Time Consume 1.865\n",
      "Epoch 152/4999 Training Loss 1096.57 Time Consume 1.846\n",
      "Epoch 153/4999 Training Loss 1095.48 Time Consume 1.886\n",
      "Epoch 154/4999 Training Loss 1094.24 Time Consume 1.911\n",
      "Epoch 155/4999 Training Loss 1093.00 Time Consume 1.942\n",
      "Epoch 156/4999 Training Loss 1091.85 Time Consume 1.916\n",
      "Epoch 157/4999 Training Loss 1090.64 Time Consume 1.897\n",
      "Epoch 158/4999 Training Loss 1089.47 Time Consume 1.885\n",
      "Epoch 159/4999 Training Loss 1088.29 Time Consume 1.896\n",
      "Epoch 160/4999 Training Loss 1087.01 Time Consume 1.922\n",
      "------------------------------------------------------------\n",
      "Evaluation 160/4999 Loss 1078.59\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 161/4999 Training Loss 1085.75 Time Consume 1.912\n",
      "Epoch 162/4999 Training Loss 1084.57 Time Consume 1.928\n",
      "Epoch 163/4999 Training Loss 1083.38 Time Consume 1.968\n",
      "Epoch 164/4999 Training Loss 1082.17 Time Consume 1.924\n",
      "Epoch 165/4999 Training Loss 1080.95 Time Consume 1.925\n",
      "Epoch 166/4999 Training Loss 1079.83 Time Consume 1.875\n",
      "Epoch 167/4999 Training Loss 1078.55 Time Consume 1.924\n",
      "Epoch 168/4999 Training Loss 1077.39 Time Consume 1.873\n",
      "Epoch 169/4999 Training Loss 1076.18 Time Consume 1.917\n",
      "Epoch 170/4999 Training Loss 1075.05 Time Consume 1.996\n",
      "------------------------------------------------------------\n",
      "Evaluation 170/4999 Loss 1066.90\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 171/4999 Training Loss 1073.81 Time Consume 1.870\n",
      "Epoch 172/4999 Training Loss 1072.56 Time Consume 1.861\n",
      "Epoch 173/4999 Training Loss 1071.44 Time Consume 1.928\n",
      "Epoch 174/4999 Training Loss 1070.22 Time Consume 1.881\n",
      "Epoch 175/4999 Training Loss 1069.01 Time Consume 1.895\n",
      "Epoch 176/4999 Training Loss 1067.79 Time Consume 1.915\n",
      "Epoch 177/4999 Training Loss 1066.58 Time Consume 1.950\n",
      "Epoch 178/4999 Training Loss 1065.40 Time Consume 1.903\n",
      "Epoch 179/4999 Training Loss 1064.13 Time Consume 1.872\n",
      "Epoch 180/4999 Training Loss 1062.91 Time Consume 1.905\n",
      "------------------------------------------------------------\n",
      "Evaluation 180/4999 Loss 1055.05\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 181/4999 Training Loss 1061.75 Time Consume 1.863\n",
      "Epoch 182/4999 Training Loss 1060.53 Time Consume 1.847\n",
      "Epoch 183/4999 Training Loss 1059.33 Time Consume 1.831\n",
      "Epoch 184/4999 Training Loss 1058.09 Time Consume 1.867\n",
      "Epoch 185/4999 Training Loss 1057.00 Time Consume 1.897\n",
      "Epoch 186/4999 Training Loss 1055.78 Time Consume 1.903\n",
      "Epoch 187/4999 Training Loss 1054.53 Time Consume 1.855\n",
      "Epoch 188/4999 Training Loss 1053.36 Time Consume 1.872\n",
      "Epoch 189/4999 Training Loss 1052.07 Time Consume 1.918\n",
      "Epoch 190/4999 Training Loss 1050.87 Time Consume 1.861\n",
      "------------------------------------------------------------\n",
      "Evaluation 190/4999 Loss 1043.24\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 191/4999 Training Loss 1049.70 Time Consume 1.879\n",
      "Epoch 192/4999 Training Loss 1048.55 Time Consume 1.918\n",
      "Epoch 193/4999 Training Loss 1047.29 Time Consume 1.899\n",
      "Epoch 194/4999 Training Loss 1046.09 Time Consume 1.905\n",
      "Epoch 195/4999 Training Loss 1044.86 Time Consume 1.919\n",
      "Epoch 196/4999 Training Loss 1043.73 Time Consume 1.875\n",
      "Epoch 197/4999 Training Loss 1042.45 Time Consume 1.843\n",
      "Epoch 198/4999 Training Loss 1041.25 Time Consume 1.909\n",
      "Epoch 199/4999 Training Loss 1040.03 Time Consume 1.903\n",
      "Epoch 200/4999 Training Loss 1038.89 Time Consume 1.859\n",
      "------------------------------------------------------------\n",
      "Evaluation 200/4999 Loss 1031.28\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 201/4999 Training Loss 1037.57 Time Consume 1.914\n",
      "Epoch 202/4999 Training Loss 1036.41 Time Consume 1.887\n",
      "Epoch 203/4999 Training Loss 1035.19 Time Consume 1.862\n",
      "Epoch 204/4999 Training Loss 1034.07 Time Consume 1.870\n",
      "Epoch 205/4999 Training Loss 1032.82 Time Consume 1.947\n",
      "Epoch 206/4999 Training Loss 1031.64 Time Consume 1.888\n",
      "Epoch 207/4999 Training Loss 1030.54 Time Consume 1.873\n",
      "Epoch 208/4999 Training Loss 1029.28 Time Consume 1.920\n",
      "Epoch 209/4999 Training Loss 1028.06 Time Consume 1.866\n",
      "Epoch 210/4999 Training Loss 1026.84 Time Consume 1.929\n",
      "------------------------------------------------------------\n",
      "Evaluation 210/4999 Loss 1019.27\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 211/4999 Training Loss 1025.61 Time Consume 1.904\n",
      "Epoch 212/4999 Training Loss 1024.48 Time Consume 1.867\n",
      "Epoch 213/4999 Training Loss 1023.19 Time Consume 1.936\n",
      "Epoch 214/4999 Training Loss 1022.11 Time Consume 1.957\n",
      "Epoch 215/4999 Training Loss 1020.82 Time Consume 1.979\n",
      "Epoch 216/4999 Training Loss 1019.58 Time Consume 1.933\n",
      "Epoch 217/4999 Training Loss 1018.35 Time Consume 1.858\n",
      "Epoch 218/4999 Training Loss 1017.15 Time Consume 1.867\n",
      "Epoch 219/4999 Training Loss 1015.98 Time Consume 1.888\n",
      "Epoch 220/4999 Training Loss 1014.80 Time Consume 1.965\n",
      "------------------------------------------------------------\n",
      "Evaluation 220/4999 Loss 1007.42\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 221/4999 Training Loss 1013.55 Time Consume 1.859\n",
      "Epoch 222/4999 Training Loss 1012.32 Time Consume 1.857\n",
      "Epoch 223/4999 Training Loss 1011.13 Time Consume 1.884\n",
      "Epoch 224/4999 Training Loss 1009.87 Time Consume 1.940\n",
      "Epoch 225/4999 Training Loss 1008.67 Time Consume 1.924\n",
      "Epoch 226/4999 Training Loss 1007.41 Time Consume 1.916\n",
      "Epoch 227/4999 Training Loss 1006.25 Time Consume 1.838\n",
      "Epoch 228/4999 Training Loss 1005.09 Time Consume 1.882\n",
      "Epoch 229/4999 Training Loss 1003.78 Time Consume 1.838\n",
      "Epoch 230/4999 Training Loss 1002.59 Time Consume 1.985\n",
      "------------------------------------------------------------\n",
      "Evaluation 230/4999 Loss 995.29\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 231/4999 Training Loss 1001.38 Time Consume 1.921\n",
      "Epoch 232/4999 Training Loss 1000.12 Time Consume 1.866\n",
      "Epoch 233/4999 Training Loss 998.98 Time Consume 1.929\n",
      "Epoch 234/4999 Training Loss 997.66 Time Consume 1.947\n",
      "Epoch 235/4999 Training Loss 996.43 Time Consume 1.870\n",
      "Epoch 236/4999 Training Loss 995.30 Time Consume 1.895\n",
      "Epoch 237/4999 Training Loss 994.05 Time Consume 1.915\n",
      "Epoch 238/4999 Training Loss 992.84 Time Consume 1.883\n",
      "Epoch 239/4999 Training Loss 991.69 Time Consume 1.854\n",
      "Epoch 240/4999 Training Loss 990.47 Time Consume 1.872\n",
      "------------------------------------------------------------\n",
      "Evaluation 240/4999 Loss 983.33\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 241/4999 Training Loss 989.36 Time Consume 1.905\n",
      "Epoch 242/4999 Training Loss 988.01 Time Consume 1.919\n",
      "Epoch 243/4999 Training Loss 986.94 Time Consume 1.797\n",
      "Epoch 244/4999 Training Loss 985.74 Time Consume 1.890\n",
      "Epoch 245/4999 Training Loss 984.57 Time Consume 2.103\n",
      "Epoch 246/4999 Training Loss 983.39 Time Consume 1.972\n",
      "Epoch 247/4999 Training Loss 982.22 Time Consume 1.891\n",
      "Epoch 248/4999 Training Loss 980.92 Time Consume 2.113\n",
      "Epoch 249/4999 Training Loss 979.78 Time Consume 1.968\n",
      "Epoch 250/4999 Training Loss 978.61 Time Consume 2.046\n",
      "------------------------------------------------------------\n",
      "Evaluation 250/4999 Loss 971.54\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 251/4999 Training Loss 977.39 Time Consume 1.994\n",
      "Epoch 252/4999 Training Loss 976.30 Time Consume 2.011\n",
      "Epoch 253/4999 Training Loss 975.12 Time Consume 1.890\n",
      "Epoch 254/4999 Training Loss 973.97 Time Consume 2.145\n",
      "Epoch 255/4999 Training Loss 972.63 Time Consume 1.848\n",
      "Epoch 256/4999 Training Loss 971.51 Time Consume 1.749\n",
      "Epoch 257/4999 Training Loss 970.40 Time Consume 1.895\n",
      "Epoch 258/4999 Training Loss 969.22 Time Consume 1.856\n",
      "Epoch 259/4999 Training Loss 968.05 Time Consume 1.897\n",
      "Epoch 260/4999 Training Loss 966.86 Time Consume 2.100\n",
      "------------------------------------------------------------\n",
      "Evaluation 260/4999 Loss 959.89\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 261/4999 Training Loss 965.71 Time Consume 2.063\n",
      "Epoch 262/4999 Training Loss 964.58 Time Consume 2.166\n",
      "Epoch 263/4999 Training Loss 963.37 Time Consume 2.238\n",
      "Epoch 264/4999 Training Loss 962.22 Time Consume 2.186\n",
      "Epoch 265/4999 Training Loss 961.14 Time Consume 2.073\n",
      "Epoch 266/4999 Training Loss 959.85 Time Consume 1.858\n",
      "Epoch 267/4999 Training Loss 958.81 Time Consume 1.889\n",
      "Epoch 268/4999 Training Loss 957.60 Time Consume 1.887\n",
      "Epoch 269/4999 Training Loss 956.34 Time Consume 2.376\n",
      "Epoch 270/4999 Training Loss 955.17 Time Consume 2.327\n",
      "------------------------------------------------------------\n",
      "Evaluation 270/4999 Loss 948.20\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 271/4999 Training Loss 953.95 Time Consume 1.995\n",
      "Epoch 272/4999 Training Loss 952.78 Time Consume 2.034\n",
      "Epoch 273/4999 Training Loss 951.60 Time Consume 2.078\n",
      "Epoch 274/4999 Training Loss 950.41 Time Consume 2.246\n",
      "Epoch 275/4999 Training Loss 949.30 Time Consume 1.953\n",
      "Epoch 276/4999 Training Loss 948.11 Time Consume 2.009\n",
      "Epoch 277/4999 Training Loss 946.88 Time Consume 1.970\n",
      "Epoch 278/4999 Training Loss 945.77 Time Consume 1.971\n",
      "Epoch 279/4999 Training Loss 944.65 Time Consume 2.018\n",
      "Epoch 280/4999 Training Loss 943.41 Time Consume 1.965\n",
      "------------------------------------------------------------\n",
      "Evaluation 280/4999 Loss 936.49\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 281/4999 Training Loss 942.25 Time Consume 1.868\n",
      "Epoch 282/4999 Training Loss 941.04 Time Consume 1.897\n",
      "Epoch 283/4999 Training Loss 939.83 Time Consume 1.985\n",
      "Epoch 284/4999 Training Loss 938.71 Time Consume 1.835\n",
      "Epoch 285/4999 Training Loss 937.57 Time Consume 2.127\n",
      "Epoch 286/4999 Training Loss 936.34 Time Consume 1.973\n",
      "Epoch 287/4999 Training Loss 935.22 Time Consume 2.206\n",
      "Epoch 288/4999 Training Loss 934.01 Time Consume 2.068\n",
      "Epoch 289/4999 Training Loss 932.84 Time Consume 2.011\n",
      "Epoch 290/4999 Training Loss 931.68 Time Consume 1.900\n",
      "------------------------------------------------------------\n",
      "Evaluation 290/4999 Loss 924.82\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 291/4999 Training Loss 930.61 Time Consume 2.042\n",
      "Epoch 292/4999 Training Loss 929.41 Time Consume 1.933\n",
      "Epoch 293/4999 Training Loss 928.28 Time Consume 1.999\n",
      "Epoch 294/4999 Training Loss 927.11 Time Consume 1.855\n",
      "Epoch 295/4999 Training Loss 926.01 Time Consume 1.857\n",
      "Epoch 296/4999 Training Loss 924.85 Time Consume 2.012\n",
      "Epoch 297/4999 Training Loss 923.74 Time Consume 1.963\n",
      "Epoch 298/4999 Training Loss 922.68 Time Consume 2.041\n",
      "Epoch 299/4999 Training Loss 921.53 Time Consume 2.107\n",
      "Epoch 300/4999 Training Loss 920.42 Time Consume 2.106\n",
      "------------------------------------------------------------\n",
      "Evaluation 300/4999 Loss 913.52\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 301/4999 Training Loss 919.37 Time Consume 2.178\n",
      "Epoch 302/4999 Training Loss 918.14 Time Consume 2.072\n",
      "Epoch 303/4999 Training Loss 917.02 Time Consume 1.912\n",
      "Epoch 304/4999 Training Loss 915.84 Time Consume 1.888\n",
      "Epoch 305/4999 Training Loss 914.90 Time Consume 1.865\n",
      "Epoch 306/4999 Training Loss 913.82 Time Consume 1.991\n",
      "Epoch 307/4999 Training Loss 912.56 Time Consume 1.884\n",
      "Epoch 308/4999 Training Loss 911.48 Time Consume 1.970\n",
      "Epoch 309/4999 Training Loss 910.29 Time Consume 1.859\n",
      "Epoch 310/4999 Training Loss 909.28 Time Consume 1.861\n",
      "------------------------------------------------------------\n",
      "Evaluation 310/4999 Loss 902.34\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 311/4999 Training Loss 908.15 Time Consume 1.968\n",
      "Epoch 312/4999 Training Loss 907.01 Time Consume 1.992\n",
      "Epoch 313/4999 Training Loss 905.94 Time Consume 1.941\n",
      "Epoch 314/4999 Training Loss 904.74 Time Consume 2.008\n",
      "Epoch 315/4999 Training Loss 903.72 Time Consume 1.810\n",
      "Epoch 316/4999 Training Loss 902.58 Time Consume 1.841\n",
      "Epoch 317/4999 Training Loss 901.52 Time Consume 1.746\n",
      "Epoch 318/4999 Training Loss 900.43 Time Consume 1.809\n",
      "Epoch 319/4999 Training Loss 899.31 Time Consume 1.882\n",
      "Epoch 320/4999 Training Loss 898.09 Time Consume 1.918\n",
      "------------------------------------------------------------\n",
      "Evaluation 320/4999 Loss 891.18\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 321/4999 Training Loss 897.05 Time Consume 1.884\n",
      "Epoch 322/4999 Training Loss 895.97 Time Consume 1.904\n",
      "Epoch 323/4999 Training Loss 894.80 Time Consume 2.037\n",
      "Epoch 324/4999 Training Loss 893.80 Time Consume 1.847\n",
      "Epoch 325/4999 Training Loss 892.63 Time Consume 2.011\n",
      "Epoch 326/4999 Training Loss 891.53 Time Consume 1.915\n",
      "Epoch 327/4999 Training Loss 890.47 Time Consume 1.833\n",
      "Epoch 328/4999 Training Loss 889.34 Time Consume 1.874\n",
      "Epoch 329/4999 Training Loss 888.27 Time Consume 1.891\n",
      "Epoch 330/4999 Training Loss 887.18 Time Consume 1.929\n",
      "------------------------------------------------------------\n",
      "Evaluation 330/4999 Loss 880.27\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 331/4999 Training Loss 886.18 Time Consume 1.913\n",
      "Epoch 332/4999 Training Loss 885.24 Time Consume 1.849\n",
      "Epoch 333/4999 Training Loss 885.10 Time Consume 1.869\n",
      "Epoch 334/4999 Training Loss 885.23 Time Consume 2.234\n",
      "Epoch 335/4999 Training Loss 885.09 Time Consume 2.057\n",
      "Epoch 336/4999 Training Loss 884.66 Time Consume 2.031\n",
      "Epoch 337/4999 Training Loss 884.22 Time Consume 2.102\n",
      "Epoch 338/4999 Training Loss 883.87 Time Consume 2.006\n",
      "Epoch 339/4999 Training Loss 883.90 Time Consume 2.134\n",
      "Epoch 340/4999 Training Loss 883.61 Time Consume 2.048\n",
      "------------------------------------------------------------\n",
      "Evaluation 340/4999 Loss 877.17\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 341/4999 Training Loss 883.28 Time Consume 1.853\n",
      "Epoch 342/4999 Training Loss 883.15 Time Consume 1.817\n",
      "Epoch 343/4999 Training Loss 882.93 Time Consume 2.148\n",
      "Epoch 344/4999 Training Loss 882.80 Time Consume 1.900\n",
      "Epoch 345/4999 Training Loss 882.61 Time Consume 1.945\n",
      "Epoch 346/4999 Training Loss 882.45 Time Consume 1.958\n",
      "Epoch 347/4999 Training Loss 882.25 Time Consume 2.240\n",
      "Epoch 348/4999 Training Loss 882.06 Time Consume 2.028\n",
      "Epoch 349/4999 Training Loss 881.83 Time Consume 2.015\n",
      "Epoch 350/4999 Training Loss 881.70 Time Consume 1.966\n",
      "------------------------------------------------------------\n",
      "Evaluation 350/4999 Loss 875.35\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 351/4999 Training Loss 881.55 Time Consume 2.094\n",
      "Epoch 352/4999 Training Loss 881.33 Time Consume 2.016\n",
      "Epoch 353/4999 Training Loss 881.23 Time Consume 2.122\n",
      "Epoch 354/4999 Training Loss 881.11 Time Consume 2.100\n",
      "Epoch 355/4999 Training Loss 880.91 Time Consume 2.049\n",
      "Epoch 356/4999 Training Loss 880.71 Time Consume 2.040\n",
      "Epoch 357/4999 Training Loss 880.68 Time Consume 2.059\n",
      "Epoch 358/4999 Training Loss 880.51 Time Consume 1.939\n",
      "Epoch 359/4999 Training Loss 880.29 Time Consume 2.000\n",
      "Epoch 360/4999 Training Loss 880.03 Time Consume 2.031\n",
      "------------------------------------------------------------\n",
      "Evaluation 360/4999 Loss 873.69\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 361/4999 Training Loss 880.01 Time Consume 1.993\n",
      "Epoch 362/4999 Training Loss 879.79 Time Consume 1.999\n",
      "Epoch 363/4999 Training Loss 879.66 Time Consume 1.909\n",
      "Epoch 364/4999 Training Loss 879.60 Time Consume 1.970\n",
      "Epoch 365/4999 Training Loss 879.45 Time Consume 1.919\n",
      "Epoch 366/4999 Training Loss 879.27 Time Consume 2.013\n",
      "Epoch 367/4999 Training Loss 879.07 Time Consume 2.156\n",
      "Epoch 368/4999 Training Loss 878.95 Time Consume 2.066\n",
      "Epoch 369/4999 Training Loss 878.81 Time Consume 2.017\n",
      "Epoch 370/4999 Training Loss 878.69 Time Consume 2.010\n",
      "------------------------------------------------------------\n",
      "Evaluation 370/4999 Loss 872.27\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 371/4999 Training Loss 878.53 Time Consume 1.900\n",
      "Epoch 372/4999 Training Loss 878.45 Time Consume 2.138\n",
      "Epoch 373/4999 Training Loss 878.26 Time Consume 1.943\n",
      "Epoch 374/4999 Training Loss 878.12 Time Consume 2.008\n",
      "Epoch 375/4999 Training Loss 878.03 Time Consume 1.963\n",
      "Epoch 376/4999 Training Loss 877.89 Time Consume 1.954\n",
      "Epoch 377/4999 Training Loss 877.81 Time Consume 1.859\n",
      "Epoch 378/4999 Training Loss 877.59 Time Consume 2.040\n",
      "Epoch 379/4999 Training Loss 877.48 Time Consume 2.045\n",
      "Epoch 380/4999 Training Loss 877.41 Time Consume 2.021\n",
      "------------------------------------------------------------\n",
      "Evaluation 380/4999 Loss 871.02\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 381/4999 Training Loss 877.25 Time Consume 1.918\n",
      "Epoch 382/4999 Training Loss 877.15 Time Consume 1.909\n",
      "Epoch 383/4999 Training Loss 877.10 Time Consume 1.842\n",
      "Epoch 384/4999 Training Loss 876.93 Time Consume 1.927\n",
      "Epoch 385/4999 Training Loss 876.89 Time Consume 1.909\n",
      "Epoch 386/4999 Training Loss 876.78 Time Consume 1.807\n",
      "Epoch 387/4999 Training Loss 876.66 Time Consume 1.763\n",
      "Epoch 388/4999 Training Loss 876.57 Time Consume 1.827\n",
      "Epoch 389/4999 Training Loss 876.45 Time Consume 1.949\n",
      "Epoch 390/4999 Training Loss 876.41 Time Consume 1.908\n",
      "------------------------------------------------------------\n",
      "Evaluation 390/4999 Loss 870.08\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 391/4999 Training Loss 876.28 Time Consume 1.863\n",
      "Epoch 392/4999 Training Loss 876.22 Time Consume 2.005\n",
      "Epoch 393/4999 Training Loss 876.08 Time Consume 1.973\n",
      "Epoch 394/4999 Training Loss 876.04 Time Consume 1.872\n",
      "Epoch 395/4999 Training Loss 875.88 Time Consume 1.945\n",
      "Epoch 396/4999 Training Loss 875.76 Time Consume 1.938\n",
      "Epoch 397/4999 Training Loss 875.66 Time Consume 1.849\n",
      "Epoch 398/4999 Training Loss 875.60 Time Consume 1.835\n",
      "Epoch 399/4999 Training Loss 875.51 Time Consume 1.826\n",
      "Epoch 400/4999 Training Loss 875.48 Time Consume 1.966\n",
      "------------------------------------------------------------\n",
      "Evaluation 400/4999 Loss 869.01\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 401/4999 Training Loss 875.47 Time Consume 1.861\n",
      "Epoch 402/4999 Training Loss 875.24 Time Consume 1.959\n",
      "Epoch 403/4999 Training Loss 875.08 Time Consume 1.741\n",
      "Epoch 404/4999 Training Loss 875.03 Time Consume 1.793\n",
      "Epoch 405/4999 Training Loss 874.94 Time Consume 1.930\n",
      "Epoch 406/4999 Training Loss 874.78 Time Consume 1.873\n",
      "Epoch 407/4999 Training Loss 874.76 Time Consume 1.815\n",
      "Epoch 408/4999 Training Loss 874.59 Time Consume 1.876\n",
      "Epoch 409/4999 Training Loss 874.57 Time Consume 1.831\n",
      "Epoch 410/4999 Training Loss 874.42 Time Consume 1.951\n",
      "------------------------------------------------------------\n",
      "Evaluation 410/4999 Loss 867.94\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 411/4999 Training Loss 874.30 Time Consume 1.940\n",
      "Epoch 412/4999 Training Loss 874.25 Time Consume 2.018\n",
      "Epoch 413/4999 Training Loss 874.15 Time Consume 2.067\n",
      "Epoch 414/4999 Training Loss 873.99 Time Consume 1.863\n",
      "Epoch 415/4999 Training Loss 873.84 Time Consume 1.907\n",
      "Epoch 416/4999 Training Loss 873.77 Time Consume 2.082\n",
      "Epoch 417/4999 Training Loss 873.73 Time Consume 2.103\n",
      "Epoch 418/4999 Training Loss 873.59 Time Consume 2.036\n",
      "Epoch 419/4999 Training Loss 873.45 Time Consume 1.870\n",
      "Epoch 420/4999 Training Loss 873.36 Time Consume 1.820\n",
      "------------------------------------------------------------\n",
      "Evaluation 420/4999 Loss 866.89\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 421/4999 Training Loss 873.27 Time Consume 1.844\n",
      "Epoch 422/4999 Training Loss 873.08 Time Consume 1.912\n",
      "Epoch 423/4999 Training Loss 873.06 Time Consume 1.910\n",
      "Epoch 424/4999 Training Loss 872.94 Time Consume 1.897\n",
      "Epoch 425/4999 Training Loss 872.88 Time Consume 1.902\n",
      "Epoch 426/4999 Training Loss 872.76 Time Consume 1.926\n",
      "Epoch 427/4999 Training Loss 872.59 Time Consume 1.913\n",
      "Epoch 428/4999 Training Loss 872.54 Time Consume 1.895\n",
      "Epoch 429/4999 Training Loss 872.39 Time Consume 1.919\n",
      "Epoch 430/4999 Training Loss 872.38 Time Consume 2.018\n",
      "------------------------------------------------------------\n",
      "Evaluation 430/4999 Loss 865.86\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 431/4999 Training Loss 872.24 Time Consume 1.927\n",
      "Epoch 432/4999 Training Loss 872.10 Time Consume 1.909\n",
      "Epoch 433/4999 Training Loss 872.06 Time Consume 1.928\n",
      "Epoch 434/4999 Training Loss 871.92 Time Consume 1.876\n",
      "Epoch 435/4999 Training Loss 871.83 Time Consume 1.964\n",
      "Epoch 436/4999 Training Loss 871.77 Time Consume 1.910\n",
      "Epoch 437/4999 Training Loss 871.70 Time Consume 1.876\n",
      "Epoch 438/4999 Training Loss 871.56 Time Consume 2.182\n",
      "Epoch 439/4999 Training Loss 871.43 Time Consume 1.984\n",
      "Epoch 440/4999 Training Loss 871.36 Time Consume 1.883\n",
      "------------------------------------------------------------\n",
      "Evaluation 440/4999 Loss 864.93\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 441/4999 Training Loss 871.27 Time Consume 1.922\n",
      "Epoch 442/4999 Training Loss 871.17 Time Consume 1.925\n",
      "Epoch 443/4999 Training Loss 871.12 Time Consume 1.773\n",
      "Epoch 444/4999 Training Loss 871.09 Time Consume 1.920\n",
      "Epoch 445/4999 Training Loss 870.94 Time Consume 1.805\n",
      "Epoch 446/4999 Training Loss 870.86 Time Consume 1.821\n",
      "Epoch 447/4999 Training Loss 870.74 Time Consume 1.782\n",
      "Epoch 448/4999 Training Loss 870.73 Time Consume 1.809\n",
      "Epoch 449/4999 Training Loss 870.55 Time Consume 1.896\n",
      "Epoch 450/4999 Training Loss 870.47 Time Consume 1.837\n",
      "------------------------------------------------------------\n",
      "Evaluation 450/4999 Loss 863.94\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 451/4999 Training Loss 870.42 Time Consume 1.842\n",
      "Epoch 452/4999 Training Loss 870.27 Time Consume 2.057\n",
      "Epoch 453/4999 Training Loss 870.21 Time Consume 1.871\n",
      "Epoch 454/4999 Training Loss 870.10 Time Consume 1.723\n",
      "Epoch 455/4999 Training Loss 869.94 Time Consume 1.743\n",
      "Epoch 456/4999 Training Loss 869.87 Time Consume 1.881\n",
      "Epoch 457/4999 Training Loss 869.86 Time Consume 1.867\n",
      "Epoch 458/4999 Training Loss 869.80 Time Consume 1.913\n",
      "Epoch 459/4999 Training Loss 869.66 Time Consume 1.864\n",
      "Epoch 460/4999 Training Loss 869.66 Time Consume 1.850\n",
      "------------------------------------------------------------\n",
      "Evaluation 460/4999 Loss 863.04\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 461/4999 Training Loss 869.53 Time Consume 1.841\n",
      "Epoch 462/4999 Training Loss 869.40 Time Consume 1.903\n",
      "Epoch 463/4999 Training Loss 869.29 Time Consume 1.857\n",
      "Epoch 464/4999 Training Loss 869.24 Time Consume 1.888\n",
      "Epoch 465/4999 Training Loss 869.19 Time Consume 1.838\n",
      "Epoch 466/4999 Training Loss 869.06 Time Consume 1.844\n",
      "Epoch 467/4999 Training Loss 869.09 Time Consume 1.844\n",
      "Epoch 468/4999 Training Loss 868.95 Time Consume 1.893\n",
      "Epoch 469/4999 Training Loss 868.87 Time Consume 1.951\n",
      "Epoch 470/4999 Training Loss 868.80 Time Consume 1.873\n",
      "------------------------------------------------------------\n",
      "Evaluation 470/4999 Loss 862.17\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 471/4999 Training Loss 868.73 Time Consume 1.968\n",
      "Epoch 472/4999 Training Loss 868.66 Time Consume 1.840\n",
      "Epoch 473/4999 Training Loss 868.56 Time Consume 1.845\n",
      "Epoch 474/4999 Training Loss 868.45 Time Consume 1.984\n",
      "Epoch 475/4999 Training Loss 868.32 Time Consume 1.841\n",
      "Epoch 476/4999 Training Loss 868.28 Time Consume 1.771\n",
      "Epoch 477/4999 Training Loss 868.27 Time Consume 1.866\n",
      "Epoch 478/4999 Training Loss 868.25 Time Consume 1.888\n",
      "Epoch 479/4999 Training Loss 868.14 Time Consume 1.823\n",
      "Epoch 480/4999 Training Loss 868.04 Time Consume 1.879\n",
      "------------------------------------------------------------\n",
      "Evaluation 480/4999 Loss 861.63\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 481/4999 Training Loss 868.07 Time Consume 1.922\n",
      "Epoch 482/4999 Training Loss 868.01 Time Consume 1.851\n",
      "Epoch 483/4999 Training Loss 867.99 Time Consume 1.859\n",
      "Epoch 484/4999 Training Loss 868.00 Time Consume 1.874\n",
      "Epoch 485/4999 Training Loss 867.82 Time Consume 1.880\n",
      "Epoch 486/4999 Training Loss 867.83 Time Consume 1.834\n",
      "Epoch 487/4999 Training Loss 867.85 Time Consume 1.762\n",
      "Epoch 488/4999 Training Loss 867.73 Time Consume 1.813\n",
      "Epoch 489/4999 Training Loss 867.83 Time Consume 1.893\n",
      "Epoch 490/4999 Training Loss 867.69 Time Consume 1.784\n",
      "------------------------------------------------------------\n",
      "Evaluation 490/4999 Loss 861.17\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 491/4999 Training Loss 867.63 Time Consume 1.802\n",
      "Epoch 492/4999 Training Loss 867.63 Time Consume 1.839\n",
      "Epoch 493/4999 Training Loss 867.55 Time Consume 1.861\n",
      "Epoch 494/4999 Training Loss 867.51 Time Consume 1.977\n",
      "Epoch 495/4999 Training Loss 867.46 Time Consume 1.751\n",
      "Epoch 496/4999 Training Loss 867.47 Time Consume 1.924\n",
      "Epoch 497/4999 Training Loss 867.42 Time Consume 1.869\n",
      "Epoch 498/4999 Training Loss 867.40 Time Consume 1.846\n",
      "Epoch 499/4999 Training Loss 867.30 Time Consume 1.845\n",
      "Epoch 500/4999 Training Loss 867.38 Time Consume 1.916\n",
      "------------------------------------------------------------\n",
      "Evaluation 500/4999 Loss 860.69\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 501/4999 Training Loss 867.24 Time Consume 1.790\n",
      "Epoch 502/4999 Training Loss 867.26 Time Consume 1.760\n",
      "Epoch 503/4999 Training Loss 867.16 Time Consume 1.883\n",
      "Epoch 504/4999 Training Loss 867.10 Time Consume 1.821\n",
      "Epoch 505/4999 Training Loss 867.13 Time Consume 1.952\n",
      "Epoch 506/4999 Training Loss 867.06 Time Consume 1.856\n",
      "Epoch 507/4999 Training Loss 867.03 Time Consume 1.830\n",
      "Epoch 508/4999 Training Loss 867.01 Time Consume 1.827\n",
      "Epoch 509/4999 Training Loss 866.94 Time Consume 1.725\n",
      "Epoch 510/4999 Training Loss 866.92 Time Consume 1.845\n",
      "------------------------------------------------------------\n",
      "Evaluation 510/4999 Loss 860.43\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 511/4999 Training Loss 866.86 Time Consume 1.823\n",
      "Epoch 512/4999 Training Loss 866.86 Time Consume 1.829\n",
      "Epoch 513/4999 Training Loss 866.83 Time Consume 1.742\n",
      "Epoch 514/4999 Training Loss 866.83 Time Consume 1.858\n",
      "Epoch 515/4999 Training Loss 866.80 Time Consume 1.868\n",
      "Epoch 516/4999 Training Loss 866.81 Time Consume 1.764\n",
      "Epoch 517/4999 Training Loss 866.72 Time Consume 1.776\n",
      "Epoch 518/4999 Training Loss 866.77 Time Consume 1.822\n",
      "Epoch 519/4999 Training Loss 866.69 Time Consume 1.883\n",
      "Epoch 520/4999 Training Loss 866.68 Time Consume 1.868\n",
      "------------------------------------------------------------\n",
      "Evaluation 520/4999 Loss 860.07\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 521/4999 Training Loss 866.63 Time Consume 1.839\n",
      "Epoch 522/4999 Training Loss 866.58 Time Consume 1.883\n",
      "Epoch 523/4999 Training Loss 866.54 Time Consume 1.833\n",
      "Epoch 524/4999 Training Loss 866.62 Time Consume 1.927\n",
      "Epoch 525/4999 Training Loss 866.50 Time Consume 1.785\n",
      "Epoch 526/4999 Training Loss 866.51 Time Consume 1.834\n",
      "Epoch 527/4999 Training Loss 866.51 Time Consume 1.871\n",
      "Epoch 528/4999 Training Loss 866.41 Time Consume 1.936\n",
      "Epoch 529/4999 Training Loss 866.49 Time Consume 1.784\n",
      "Epoch 530/4999 Training Loss 866.40 Time Consume 1.834\n",
      "------------------------------------------------------------\n",
      "Evaluation 530/4999 Loss 859.84\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 531/4999 Training Loss 866.37 Time Consume 1.983\n",
      "Epoch 532/4999 Training Loss 866.35 Time Consume 1.933\n",
      "Epoch 533/4999 Training Loss 866.32 Time Consume 1.843\n",
      "Epoch 534/4999 Training Loss 866.33 Time Consume 1.908\n",
      "Epoch 535/4999 Training Loss 866.24 Time Consume 1.889\n",
      "Epoch 536/4999 Training Loss 866.20 Time Consume 1.835\n",
      "Epoch 537/4999 Training Loss 866.22 Time Consume 1.891\n",
      "Epoch 538/4999 Training Loss 866.22 Time Consume 2.015\n",
      "Epoch 539/4999 Training Loss 866.18 Time Consume 1.850\n",
      "Epoch 540/4999 Training Loss 866.14 Time Consume 1.877\n",
      "------------------------------------------------------------\n",
      "Evaluation 540/4999 Loss 859.49\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 541/4999 Training Loss 866.05 Time Consume 1.836\n",
      "Epoch 542/4999 Training Loss 866.04 Time Consume 1.983\n",
      "Epoch 543/4999 Training Loss 866.02 Time Consume 1.935\n",
      "Epoch 544/4999 Training Loss 866.05 Time Consume 1.959\n",
      "Epoch 545/4999 Training Loss 866.03 Time Consume 1.918\n",
      "Epoch 546/4999 Training Loss 865.97 Time Consume 1.860\n",
      "Epoch 547/4999 Training Loss 865.95 Time Consume 1.889\n",
      "Epoch 548/4999 Training Loss 865.90 Time Consume 1.840\n",
      "Epoch 549/4999 Training Loss 865.85 Time Consume 1.909\n",
      "Epoch 550/4999 Training Loss 865.87 Time Consume 1.925\n",
      "------------------------------------------------------------\n",
      "Evaluation 550/4999 Loss 859.26\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 551/4999 Training Loss 865.91 Time Consume 1.925\n",
      "Epoch 552/4999 Training Loss 865.79 Time Consume 1.941\n",
      "Epoch 553/4999 Training Loss 865.80 Time Consume 1.937\n",
      "Epoch 554/4999 Training Loss 865.81 Time Consume 1.920\n",
      "Epoch 555/4999 Training Loss 865.76 Time Consume 1.951\n",
      "Epoch 556/4999 Training Loss 865.70 Time Consume 1.886\n",
      "Epoch 557/4999 Training Loss 865.68 Time Consume 1.830\n",
      "Epoch 558/4999 Training Loss 865.63 Time Consume 1.855\n",
      "Epoch 559/4999 Training Loss 865.63 Time Consume 1.865\n",
      "Epoch 560/4999 Training Loss 865.59 Time Consume 1.839\n",
      "------------------------------------------------------------\n",
      "Evaluation 560/4999 Loss 858.93\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 561/4999 Training Loss 865.63 Time Consume 1.850\n",
      "Epoch 562/4999 Training Loss 865.56 Time Consume 1.933\n",
      "Epoch 563/4999 Training Loss 865.52 Time Consume 1.800\n",
      "Epoch 564/4999 Training Loss 865.52 Time Consume 1.832\n",
      "Epoch 565/4999 Training Loss 865.48 Time Consume 1.901\n",
      "Epoch 566/4999 Training Loss 865.43 Time Consume 1.847\n",
      "Epoch 567/4999 Training Loss 865.39 Time Consume 1.934\n",
      "Epoch 568/4999 Training Loss 865.35 Time Consume 1.775\n",
      "Epoch 569/4999 Training Loss 865.38 Time Consume 1.880\n",
      "Epoch 570/4999 Training Loss 865.37 Time Consume 1.814\n",
      "------------------------------------------------------------\n",
      "Evaluation 570/4999 Loss 858.69\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 571/4999 Training Loss 865.36 Time Consume 1.903\n",
      "Epoch 572/4999 Training Loss 865.29 Time Consume 1.743\n",
      "Epoch 573/4999 Training Loss 865.25 Time Consume 1.852\n",
      "Epoch 574/4999 Training Loss 865.19 Time Consume 1.857\n",
      "Epoch 575/4999 Training Loss 865.15 Time Consume 1.872\n",
      "Epoch 576/4999 Training Loss 865.17 Time Consume 2.026\n",
      "Epoch 577/4999 Training Loss 865.16 Time Consume 1.928\n",
      "Epoch 578/4999 Training Loss 865.11 Time Consume 1.904\n",
      "Epoch 579/4999 Training Loss 865.06 Time Consume 1.926\n",
      "Epoch 580/4999 Training Loss 865.09 Time Consume 1.833\n",
      "------------------------------------------------------------\n",
      "Evaluation 580/4999 Loss 858.40\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 581/4999 Training Loss 865.04 Time Consume 1.870\n",
      "Epoch 582/4999 Training Loss 865.04 Time Consume 1.879\n",
      "Epoch 583/4999 Training Loss 864.97 Time Consume 1.848\n",
      "Epoch 584/4999 Training Loss 864.93 Time Consume 1.854\n",
      "Epoch 585/4999 Training Loss 864.92 Time Consume 1.859\n",
      "Epoch 586/4999 Training Loss 864.94 Time Consume 1.797\n",
      "Epoch 587/4999 Training Loss 864.96 Time Consume 1.902\n",
      "Epoch 588/4999 Training Loss 864.81 Time Consume 2.032\n",
      "Epoch 589/4999 Training Loss 864.86 Time Consume 1.891\n",
      "Epoch 590/4999 Training Loss 864.86 Time Consume 2.113\n",
      "------------------------------------------------------------\n",
      "Evaluation 590/4999 Loss 858.14\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 591/4999 Training Loss 864.82 Time Consume 1.707\n",
      "Epoch 592/4999 Training Loss 864.77 Time Consume 1.786\n",
      "Epoch 593/4999 Training Loss 864.81 Time Consume 1.880\n",
      "Epoch 594/4999 Training Loss 864.86 Time Consume 1.855\n",
      "Epoch 595/4999 Training Loss 864.76 Time Consume 1.889\n",
      "Epoch 596/4999 Training Loss 864.72 Time Consume 1.879\n",
      "Epoch 597/4999 Training Loss 864.72 Time Consume 1.832\n",
      "Epoch 598/4999 Training Loss 864.77 Time Consume 1.833\n",
      "Epoch 599/4999 Training Loss 864.74 Time Consume 1.868\n",
      "Epoch 600/4999 Training Loss 864.80 Time Consume 1.744\n",
      "------------------------------------------------------------\n",
      "Evaluation 600/4999 Loss 858.04\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 601/4999 Training Loss 864.72 Time Consume 1.936\n",
      "Epoch 602/4999 Training Loss 864.71 Time Consume 1.958\n",
      "Epoch 603/4999 Training Loss 864.76 Time Consume 1.894\n",
      "Epoch 604/4999 Training Loss 864.69 Time Consume 1.973\n",
      "Epoch 605/4999 Training Loss 864.67 Time Consume 1.880\n",
      "Epoch 606/4999 Training Loss 864.74 Time Consume 1.850\n",
      "Epoch 607/4999 Training Loss 864.65 Time Consume 1.860\n",
      "Epoch 608/4999 Training Loss 864.71 Time Consume 1.852\n",
      "Epoch 609/4999 Training Loss 864.65 Time Consume 1.890\n",
      "Epoch 610/4999 Training Loss 864.64 Time Consume 1.851\n",
      "------------------------------------------------------------\n",
      "Evaluation 610/4999 Loss 858.07\n",
      "------------------------------------------------------------\n",
      "Epoch 611/4999 Training Loss 864.66 Time Consume 1.826\n",
      "Epoch 612/4999 Training Loss 864.65 Time Consume 1.913\n",
      "Epoch 613/4999 Training Loss 864.63 Time Consume 2.104\n",
      "Epoch 614/4999 Training Loss 864.61 Time Consume 1.902\n",
      "Epoch 615/4999 Training Loss 864.61 Time Consume 2.055\n",
      "Epoch 616/4999 Training Loss 864.63 Time Consume 1.935\n",
      "Epoch 617/4999 Training Loss 864.65 Time Consume 1.852\n",
      "Epoch 618/4999 Training Loss 864.62 Time Consume 1.845\n",
      "Epoch 619/4999 Training Loss 864.57 Time Consume 1.883\n",
      "Epoch 620/4999 Training Loss 864.59 Time Consume 1.851\n",
      "------------------------------------------------------------\n",
      "Evaluation 620/4999 Loss 857.88\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 621/4999 Training Loss 864.58 Time Consume 1.839\n",
      "Epoch 622/4999 Training Loss 864.55 Time Consume 1.860\n",
      "Epoch 623/4999 Training Loss 864.52 Time Consume 1.863\n",
      "Epoch 624/4999 Training Loss 864.54 Time Consume 1.898\n",
      "Epoch 625/4999 Training Loss 864.59 Time Consume 1.883\n",
      "Epoch 626/4999 Training Loss 864.56 Time Consume 1.895\n",
      "Epoch 627/4999 Training Loss 864.58 Time Consume 2.012\n",
      "Epoch 628/4999 Training Loss 864.51 Time Consume 1.899\n",
      "Epoch 629/4999 Training Loss 864.52 Time Consume 1.898\n",
      "Epoch 630/4999 Training Loss 864.56 Time Consume 1.970\n",
      "------------------------------------------------------------\n",
      "Evaluation 630/4999 Loss 857.82\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 631/4999 Training Loss 864.47 Time Consume 2.035\n",
      "Epoch 632/4999 Training Loss 864.44 Time Consume 1.932\n",
      "Epoch 633/4999 Training Loss 864.53 Time Consume 1.879\n",
      "Epoch 634/4999 Training Loss 864.51 Time Consume 2.058\n",
      "Epoch 635/4999 Training Loss 864.57 Time Consume 1.895\n",
      "Epoch 636/4999 Training Loss 864.57 Time Consume 1.817\n",
      "Epoch 637/4999 Training Loss 864.45 Time Consume 1.947\n",
      "Epoch 638/4999 Training Loss 864.46 Time Consume 1.805\n",
      "Epoch 639/4999 Training Loss 864.47 Time Consume 1.832\n",
      "Epoch 640/4999 Training Loss 864.40 Time Consume 1.872\n",
      "------------------------------------------------------------\n",
      "Evaluation 640/4999 Loss 857.71\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 641/4999 Training Loss 864.49 Time Consume 1.836\n",
      "Epoch 642/4999 Training Loss 864.38 Time Consume 1.856\n",
      "Epoch 643/4999 Training Loss 864.44 Time Consume 1.854\n",
      "Epoch 644/4999 Training Loss 864.52 Time Consume 1.966\n",
      "Epoch 645/4999 Training Loss 864.39 Time Consume 1.940\n",
      "Epoch 646/4999 Training Loss 864.41 Time Consume 1.871\n",
      "Epoch 647/4999 Training Loss 864.43 Time Consume 1.878\n",
      "Epoch 648/4999 Training Loss 864.49 Time Consume 1.925\n",
      "Epoch 649/4999 Training Loss 864.44 Time Consume 1.882\n",
      "Epoch 650/4999 Training Loss 864.40 Time Consume 2.009\n",
      "------------------------------------------------------------\n",
      "Evaluation 650/4999 Loss 857.68\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 651/4999 Training Loss 864.53 Time Consume 1.839\n",
      "Epoch 652/4999 Training Loss 864.49 Time Consume 1.884\n",
      "Epoch 653/4999 Training Loss 864.41 Time Consume 1.944\n",
      "Epoch 654/4999 Training Loss 864.46 Time Consume 1.963\n",
      "Epoch 655/4999 Training Loss 864.40 Time Consume 1.896\n",
      "Epoch 656/4999 Training Loss 864.41 Time Consume 1.855\n",
      "Epoch 657/4999 Training Loss 864.45 Time Consume 1.858\n",
      "Epoch 658/4999 Training Loss 864.42 Time Consume 1.802\n",
      "Epoch 659/4999 Training Loss 864.33 Time Consume 1.742\n",
      "Epoch 660/4999 Training Loss 864.37 Time Consume 1.878\n",
      "------------------------------------------------------------\n",
      "Evaluation 660/4999 Loss 857.60\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 661/4999 Training Loss 864.52 Time Consume 1.868\n",
      "Epoch 662/4999 Training Loss 864.37 Time Consume 1.956\n",
      "Epoch 663/4999 Training Loss 864.37 Time Consume 1.870\n",
      "Epoch 664/4999 Training Loss 864.38 Time Consume 1.915\n",
      "Epoch 665/4999 Training Loss 864.35 Time Consume 1.916\n",
      "Epoch 666/4999 Training Loss 864.34 Time Consume 2.091\n",
      "Epoch 667/4999 Training Loss 864.41 Time Consume 1.885\n",
      "Epoch 668/4999 Training Loss 864.33 Time Consume 1.862\n",
      "Epoch 669/4999 Training Loss 864.37 Time Consume 1.879\n",
      "Epoch 670/4999 Training Loss 864.43 Time Consume 1.844\n",
      "------------------------------------------------------------\n",
      "Evaluation 670/4999 Loss 857.63\n",
      "------------------------------------------------------------\n",
      "Epoch 671/4999 Training Loss 864.34 Time Consume 1.837\n",
      "Epoch 672/4999 Training Loss 864.30 Time Consume 1.843\n",
      "Epoch 673/4999 Training Loss 864.35 Time Consume 1.967\n",
      "Epoch 674/4999 Training Loss 864.36 Time Consume 1.846\n",
      "Epoch 675/4999 Training Loss 864.34 Time Consume 1.858\n",
      "Epoch 676/4999 Training Loss 864.32 Time Consume 1.889\n",
      "Epoch 677/4999 Training Loss 864.31 Time Consume 1.845\n",
      "Epoch 678/4999 Training Loss 864.32 Time Consume 1.886\n",
      "Epoch 679/4999 Training Loss 864.29 Time Consume 1.845\n",
      "Epoch 680/4999 Training Loss 864.28 Time Consume 1.901\n",
      "------------------------------------------------------------\n",
      "Evaluation 680/4999 Loss 857.51\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 681/4999 Training Loss 864.33 Time Consume 1.876\n",
      "Epoch 682/4999 Training Loss 864.26 Time Consume 1.843\n",
      "Epoch 683/4999 Training Loss 864.26 Time Consume 1.850\n",
      "Epoch 684/4999 Training Loss 864.29 Time Consume 1.941\n",
      "Epoch 685/4999 Training Loss 864.29 Time Consume 1.945\n",
      "Epoch 686/4999 Training Loss 864.26 Time Consume 1.851\n",
      "Epoch 687/4999 Training Loss 864.42 Time Consume 1.914\n",
      "Epoch 688/4999 Training Loss 864.24 Time Consume 1.948\n",
      "Epoch 689/4999 Training Loss 864.23 Time Consume 1.861\n",
      "Epoch 690/4999 Training Loss 864.34 Time Consume 1.887\n",
      "------------------------------------------------------------\n",
      "Evaluation 690/4999 Loss 857.39\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 691/4999 Training Loss 864.18 Time Consume 1.872\n",
      "Epoch 692/4999 Training Loss 864.33 Time Consume 1.849\n",
      "Epoch 693/4999 Training Loss 864.23 Time Consume 1.851\n",
      "Epoch 694/4999 Training Loss 864.25 Time Consume 1.759\n",
      "Epoch 695/4999 Training Loss 864.23 Time Consume 1.882\n",
      "Epoch 696/4999 Training Loss 864.18 Time Consume 1.829\n",
      "Epoch 697/4999 Training Loss 864.21 Time Consume 1.875\n",
      "Epoch 698/4999 Training Loss 864.22 Time Consume 1.966\n",
      "Epoch 699/4999 Training Loss 864.16 Time Consume 1.797\n",
      "Epoch 700/4999 Training Loss 864.21 Time Consume 1.756\n",
      "------------------------------------------------------------\n",
      "Evaluation 700/4999 Loss 857.37\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 701/4999 Training Loss 864.16 Time Consume 1.868\n",
      "Epoch 702/4999 Training Loss 864.17 Time Consume 1.875\n",
      "Epoch 703/4999 Training Loss 864.23 Time Consume 1.846\n",
      "Epoch 704/4999 Training Loss 864.18 Time Consume 1.817\n",
      "Epoch 705/4999 Training Loss 864.15 Time Consume 1.946\n",
      "Epoch 706/4999 Training Loss 864.17 Time Consume 1.706\n",
      "Epoch 707/4999 Training Loss 864.13 Time Consume 1.835\n",
      "Epoch 708/4999 Training Loss 864.20 Time Consume 1.843\n",
      "Epoch 709/4999 Training Loss 864.14 Time Consume 1.843\n",
      "Epoch 710/4999 Training Loss 864.12 Time Consume 1.873\n",
      "------------------------------------------------------------\n",
      "Evaluation 710/4999 Loss 857.37\n",
      "------------------------------------------------------------\n",
      "Epoch 711/4999 Training Loss 864.14 Time Consume 1.860\n",
      "Epoch 712/4999 Training Loss 864.16 Time Consume 1.845\n",
      "Epoch 713/4999 Training Loss 864.24 Time Consume 1.853\n",
      "Epoch 714/4999 Training Loss 864.19 Time Consume 1.875\n",
      "Epoch 715/4999 Training Loss 864.12 Time Consume 1.875\n",
      "Epoch 716/4999 Training Loss 864.20 Time Consume 1.824\n",
      "Epoch 717/4999 Training Loss 864.10 Time Consume 1.822\n",
      "Epoch 718/4999 Training Loss 864.17 Time Consume 1.838\n",
      "Epoch 719/4999 Training Loss 864.07 Time Consume 1.879\n",
      "Epoch 720/4999 Training Loss 864.17 Time Consume 1.858\n",
      "------------------------------------------------------------\n",
      "Evaluation 720/4999 Loss 857.34\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 721/4999 Training Loss 864.15 Time Consume 1.873\n",
      "Epoch 722/4999 Training Loss 864.10 Time Consume 1.864\n",
      "Epoch 723/4999 Training Loss 864.14 Time Consume 1.873\n",
      "Epoch 724/4999 Training Loss 864.10 Time Consume 1.900\n",
      "Epoch 725/4999 Training Loss 864.10 Time Consume 1.908\n",
      "Epoch 726/4999 Training Loss 864.15 Time Consume 1.709\n",
      "Epoch 727/4999 Training Loss 864.08 Time Consume 1.767\n",
      "Epoch 728/4999 Training Loss 864.10 Time Consume 1.820\n",
      "Epoch 729/4999 Training Loss 864.17 Time Consume 1.821\n",
      "Epoch 730/4999 Training Loss 864.09 Time Consume 1.861\n",
      "------------------------------------------------------------\n",
      "Evaluation 730/4999 Loss 857.30\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 731/4999 Training Loss 864.11 Time Consume 1.821\n",
      "Epoch 732/4999 Training Loss 864.13 Time Consume 1.855\n",
      "Epoch 733/4999 Training Loss 864.04 Time Consume 1.868\n",
      "Epoch 734/4999 Training Loss 864.01 Time Consume 1.846\n",
      "Epoch 735/4999 Training Loss 864.07 Time Consume 1.861\n",
      "Epoch 736/4999 Training Loss 864.00 Time Consume 1.831\n",
      "Epoch 737/4999 Training Loss 863.98 Time Consume 1.783\n",
      "Epoch 738/4999 Training Loss 864.00 Time Consume 1.687\n",
      "Epoch 739/4999 Training Loss 863.99 Time Consume 1.693\n",
      "Epoch 740/4999 Training Loss 863.99 Time Consume 1.698\n",
      "------------------------------------------------------------\n",
      "Evaluation 740/4999 Loss 857.26\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 741/4999 Training Loss 864.08 Time Consume 1.689\n",
      "Epoch 742/4999 Training Loss 864.02 Time Consume 1.773\n",
      "Epoch 743/4999 Training Loss 863.97 Time Consume 1.902\n",
      "Epoch 744/4999 Training Loss 864.02 Time Consume 1.888\n",
      "Epoch 745/4999 Training Loss 864.07 Time Consume 1.846\n",
      "Epoch 746/4999 Training Loss 863.93 Time Consume 1.842\n",
      "Epoch 747/4999 Training Loss 864.00 Time Consume 1.853\n",
      "Epoch 748/4999 Training Loss 864.02 Time Consume 1.826\n",
      "Epoch 749/4999 Training Loss 863.92 Time Consume 1.896\n",
      "Epoch 750/4999 Training Loss 864.01 Time Consume 1.886\n",
      "------------------------------------------------------------\n",
      "Evaluation 750/4999 Loss 857.20\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 751/4999 Training Loss 863.95 Time Consume 1.889\n",
      "Epoch 752/4999 Training Loss 863.91 Time Consume 1.825\n",
      "Epoch 753/4999 Training Loss 863.98 Time Consume 1.811\n",
      "Epoch 754/4999 Training Loss 863.97 Time Consume 1.738\n",
      "Epoch 755/4999 Training Loss 863.93 Time Consume 1.888\n",
      "Epoch 756/4999 Training Loss 863.95 Time Consume 1.881\n",
      "Epoch 757/4999 Training Loss 864.00 Time Consume 1.878\n",
      "Epoch 758/4999 Training Loss 864.03 Time Consume 1.812\n",
      "Epoch 759/4999 Training Loss 863.97 Time Consume 1.967\n",
      "Epoch 760/4999 Training Loss 863.88 Time Consume 1.761\n",
      "------------------------------------------------------------\n",
      "Evaluation 760/4999 Loss 857.10\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 761/4999 Training Loss 863.95 Time Consume 1.943\n",
      "Epoch 762/4999 Training Loss 863.94 Time Consume 1.932\n",
      "Epoch 763/4999 Training Loss 863.92 Time Consume 1.926\n",
      "Epoch 764/4999 Training Loss 864.00 Time Consume 1.817\n",
      "Epoch 765/4999 Training Loss 863.99 Time Consume 1.768\n",
      "Epoch 766/4999 Training Loss 863.98 Time Consume 1.863\n",
      "Epoch 767/4999 Training Loss 863.93 Time Consume 1.922\n",
      "Epoch 768/4999 Training Loss 863.90 Time Consume 1.892\n",
      "Epoch 769/4999 Training Loss 863.94 Time Consume 1.853\n",
      "Epoch 770/4999 Training Loss 863.90 Time Consume 1.817\n",
      "------------------------------------------------------------\n",
      "Evaluation 770/4999 Loss 857.09\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 771/4999 Training Loss 863.94 Time Consume 1.868\n",
      "Epoch 772/4999 Training Loss 863.97 Time Consume 1.880\n",
      "Epoch 773/4999 Training Loss 863.86 Time Consume 1.839\n",
      "Epoch 774/4999 Training Loss 863.89 Time Consume 1.957\n",
      "Epoch 775/4999 Training Loss 863.90 Time Consume 1.853\n",
      "Epoch 776/4999 Training Loss 863.83 Time Consume 1.852\n",
      "Epoch 777/4999 Training Loss 863.92 Time Consume 1.856\n",
      "Epoch 778/4999 Training Loss 863.93 Time Consume 1.864\n",
      "Epoch 779/4999 Training Loss 863.88 Time Consume 1.814\n",
      "Epoch 780/4999 Training Loss 863.90 Time Consume 1.782\n",
      "------------------------------------------------------------\n",
      "Evaluation 780/4999 Loss 857.10\n",
      "------------------------------------------------------------\n",
      "Epoch 781/4999 Training Loss 863.87 Time Consume 1.970\n",
      "Epoch 782/4999 Training Loss 863.89 Time Consume 1.823\n",
      "Epoch 783/4999 Training Loss 863.92 Time Consume 1.822\n",
      "Epoch 784/4999 Training Loss 863.85 Time Consume 1.966\n",
      "Epoch 785/4999 Training Loss 863.93 Time Consume 1.879\n",
      "Epoch 786/4999 Training Loss 863.96 Time Consume 1.844\n",
      "Epoch 787/4999 Training Loss 863.96 Time Consume 1.841\n",
      "Epoch 788/4999 Training Loss 864.06 Time Consume 1.814\n",
      "Epoch 789/4999 Training Loss 863.80 Time Consume 1.928\n",
      "Epoch 790/4999 Training Loss 863.91 Time Consume 1.855\n",
      "------------------------------------------------------------\n",
      "Evaluation 790/4999 Loss 857.13\n",
      "------------------------------------------------------------\n",
      "Epoch 791/4999 Training Loss 863.81 Time Consume 1.860\n",
      "Epoch 792/4999 Training Loss 863.88 Time Consume 1.884\n",
      "Epoch 793/4999 Training Loss 863.90 Time Consume 1.926\n",
      "Epoch 794/4999 Training Loss 863.87 Time Consume 1.909\n",
      "Epoch 795/4999 Training Loss 863.97 Time Consume 1.837\n",
      "Epoch 796/4999 Training Loss 863.83 Time Consume 1.854\n",
      "Epoch 797/4999 Training Loss 863.85 Time Consume 1.863\n",
      "Epoch 798/4999 Training Loss 863.87 Time Consume 1.780\n",
      "Epoch 799/4999 Training Loss 863.89 Time Consume 1.761\n",
      "Epoch 800/4999 Training Loss 863.83 Time Consume 1.952\n",
      "------------------------------------------------------------\n",
      "Evaluation 800/4999 Loss 857.09\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 801/4999 Training Loss 863.84 Time Consume 1.858\n",
      "Epoch 802/4999 Training Loss 863.79 Time Consume 1.836\n",
      "Epoch 803/4999 Training Loss 863.82 Time Consume 1.913\n",
      "Epoch 804/4999 Training Loss 863.83 Time Consume 1.854\n",
      "Epoch 805/4999 Training Loss 863.78 Time Consume 1.883\n",
      "Epoch 806/4999 Training Loss 863.85 Time Consume 2.069\n",
      "Epoch 807/4999 Training Loss 863.82 Time Consume 1.830\n",
      "Epoch 808/4999 Training Loss 863.84 Time Consume 1.876\n",
      "Epoch 809/4999 Training Loss 863.85 Time Consume 2.170\n",
      "Epoch 810/4999 Training Loss 863.83 Time Consume 1.846\n",
      "------------------------------------------------------------\n",
      "Evaluation 810/4999 Loss 857.02\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 811/4999 Training Loss 863.79 Time Consume 1.852\n",
      "Epoch 812/4999 Training Loss 863.89 Time Consume 1.818\n",
      "Epoch 813/4999 Training Loss 863.80 Time Consume 1.799\n",
      "Epoch 814/4999 Training Loss 863.78 Time Consume 1.728\n",
      "Epoch 815/4999 Training Loss 863.77 Time Consume 1.737\n",
      "Epoch 816/4999 Training Loss 863.82 Time Consume 1.756\n",
      "Epoch 817/4999 Training Loss 863.77 Time Consume 1.872\n",
      "Epoch 818/4999 Training Loss 863.83 Time Consume 1.918\n",
      "Epoch 819/4999 Training Loss 863.79 Time Consume 1.881\n",
      "Epoch 820/4999 Training Loss 863.82 Time Consume 1.928\n",
      "------------------------------------------------------------\n",
      "Evaluation 820/4999 Loss 856.89\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 821/4999 Training Loss 863.72 Time Consume 1.902\n",
      "Epoch 822/4999 Training Loss 863.74 Time Consume 1.902\n",
      "Epoch 823/4999 Training Loss 863.75 Time Consume 1.900\n",
      "Epoch 824/4999 Training Loss 863.81 Time Consume 1.855\n",
      "Epoch 825/4999 Training Loss 863.85 Time Consume 1.853\n",
      "Epoch 826/4999 Training Loss 863.81 Time Consume 1.859\n",
      "Epoch 827/4999 Training Loss 863.76 Time Consume 1.999\n",
      "Epoch 828/4999 Training Loss 863.79 Time Consume 1.924\n",
      "Epoch 829/4999 Training Loss 863.72 Time Consume 1.976\n",
      "Epoch 830/4999 Training Loss 863.66 Time Consume 1.764\n",
      "------------------------------------------------------------\n",
      "Evaluation 830/4999 Loss 856.91\n",
      "------------------------------------------------------------\n",
      "Epoch 831/4999 Training Loss 863.83 Time Consume 1.942\n",
      "Epoch 832/4999 Training Loss 863.89 Time Consume 1.917\n",
      "Epoch 833/4999 Training Loss 863.72 Time Consume 1.808\n",
      "Epoch 834/4999 Training Loss 863.78 Time Consume 1.783\n",
      "Epoch 835/4999 Training Loss 863.75 Time Consume 1.959\n",
      "Epoch 836/4999 Training Loss 863.73 Time Consume 2.032\n",
      "Epoch 837/4999 Training Loss 863.75 Time Consume 1.888\n",
      "Epoch 838/4999 Training Loss 863.72 Time Consume 1.913\n",
      "Epoch 839/4999 Training Loss 863.73 Time Consume 1.936\n",
      "Epoch 840/4999 Training Loss 863.73 Time Consume 1.893\n",
      "------------------------------------------------------------\n",
      "Evaluation 840/4999 Loss 856.90\n",
      "------------------------------------------------------------\n",
      "Epoch 841/4999 Training Loss 863.75 Time Consume 1.866\n",
      "Epoch 842/4999 Training Loss 863.85 Time Consume 1.922\n",
      "Epoch 843/4999 Training Loss 863.85 Time Consume 1.748\n",
      "Epoch 844/4999 Training Loss 863.80 Time Consume 1.845\n",
      "Epoch 845/4999 Training Loss 863.71 Time Consume 1.905\n",
      "Epoch 846/4999 Training Loss 863.77 Time Consume 1.902\n",
      "Epoch 847/4999 Training Loss 863.74 Time Consume 1.870\n",
      "Epoch 848/4999 Training Loss 863.68 Time Consume 1.841\n",
      "Epoch 849/4999 Training Loss 863.78 Time Consume 1.863\n",
      "Epoch 850/4999 Training Loss 863.82 Time Consume 1.951\n",
      "------------------------------------------------------------\n",
      "Evaluation 850/4999 Loss 856.97\n",
      "------------------------------------------------------------\n",
      "Epoch 851/4999 Training Loss 863.74 Time Consume 1.915\n",
      "Epoch 852/4999 Training Loss 863.82 Time Consume 1.845\n",
      "Epoch 853/4999 Training Loss 863.75 Time Consume 1.861\n",
      "Epoch 854/4999 Training Loss 863.71 Time Consume 1.850\n",
      "Epoch 855/4999 Training Loss 863.69 Time Consume 1.854\n",
      "Epoch 856/4999 Training Loss 863.65 Time Consume 2.040\n",
      "Epoch 857/4999 Training Loss 863.68 Time Consume 1.871\n",
      "Epoch 858/4999 Training Loss 863.70 Time Consume 1.859\n",
      "Epoch 859/4999 Training Loss 863.66 Time Consume 2.333\n",
      "Epoch 860/4999 Training Loss 863.68 Time Consume 2.172\n",
      "------------------------------------------------------------\n",
      "Evaluation 860/4999 Loss 856.88\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 861/4999 Training Loss 863.72 Time Consume 1.875\n",
      "Epoch 862/4999 Training Loss 863.73 Time Consume 1.852\n",
      "Epoch 863/4999 Training Loss 863.68 Time Consume 1.830\n",
      "Epoch 864/4999 Training Loss 863.72 Time Consume 1.847\n",
      "Epoch 865/4999 Training Loss 863.76 Time Consume 1.872\n",
      "Epoch 866/4999 Training Loss 863.69 Time Consume 1.863\n",
      "Epoch 867/4999 Training Loss 863.70 Time Consume 1.819\n",
      "Epoch 868/4999 Training Loss 863.70 Time Consume 1.853\n",
      "Epoch 869/4999 Training Loss 863.68 Time Consume 1.858\n",
      "Epoch 870/4999 Training Loss 863.68 Time Consume 1.838\n",
      "------------------------------------------------------------\n",
      "Evaluation 870/4999 Loss 856.88\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 871/4999 Training Loss 863.67 Time Consume 1.837\n",
      "Epoch 872/4999 Training Loss 863.68 Time Consume 1.855\n",
      "Epoch 873/4999 Training Loss 863.68 Time Consume 1.889\n",
      "Epoch 874/4999 Training Loss 863.69 Time Consume 1.866\n",
      "Epoch 875/4999 Training Loss 863.66 Time Consume 1.831\n",
      "Epoch 876/4999 Training Loss 863.67 Time Consume 1.822\n",
      "Epoch 877/4999 Training Loss 863.65 Time Consume 1.838\n",
      "Epoch 878/4999 Training Loss 863.68 Time Consume 1.828\n",
      "Epoch 879/4999 Training Loss 863.67 Time Consume 1.818\n",
      "Epoch 880/4999 Training Loss 863.71 Time Consume 1.692\n",
      "------------------------------------------------------------\n",
      "Evaluation 880/4999 Loss 856.84\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 881/4999 Training Loss 863.63 Time Consume 1.807\n",
      "Epoch 882/4999 Training Loss 863.66 Time Consume 1.836\n",
      "Epoch 883/4999 Training Loss 863.76 Time Consume 1.826\n",
      "Epoch 884/4999 Training Loss 863.73 Time Consume 1.840\n",
      "Epoch 885/4999 Training Loss 863.64 Time Consume 1.828\n",
      "Epoch 886/4999 Training Loss 863.63 Time Consume 1.831\n",
      "Epoch 887/4999 Training Loss 863.62 Time Consume 1.846\n",
      "Epoch 888/4999 Training Loss 863.67 Time Consume 1.811\n",
      "Epoch 889/4999 Training Loss 863.60 Time Consume 1.848\n",
      "Epoch 890/4999 Training Loss 863.79 Time Consume 1.841\n",
      "------------------------------------------------------------\n",
      "Evaluation 890/4999 Loss 856.89\n",
      "------------------------------------------------------------\n",
      "Epoch 891/4999 Training Loss 863.69 Time Consume 1.777\n",
      "Epoch 892/4999 Training Loss 863.65 Time Consume 1.972\n",
      "Epoch 893/4999 Training Loss 863.60 Time Consume 2.104\n",
      "Epoch 894/4999 Training Loss 863.62 Time Consume 1.843\n",
      "Epoch 895/4999 Training Loss 863.72 Time Consume 1.826\n",
      "Epoch 896/4999 Training Loss 863.62 Time Consume 1.828\n",
      "Epoch 897/4999 Training Loss 863.65 Time Consume 1.830\n",
      "Epoch 898/4999 Training Loss 863.62 Time Consume 1.824\n",
      "Epoch 899/4999 Training Loss 863.66 Time Consume 1.852\n",
      "Epoch 900/4999 Training Loss 863.71 Time Consume 1.852\n",
      "------------------------------------------------------------\n",
      "Evaluation 900/4999 Loss 856.73\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 901/4999 Training Loss 863.59 Time Consume 1.842\n",
      "Epoch 902/4999 Training Loss 863.64 Time Consume 1.815\n",
      "Epoch 903/4999 Training Loss 863.69 Time Consume 1.858\n",
      "Epoch 904/4999 Training Loss 863.74 Time Consume 1.837\n",
      "Epoch 905/4999 Training Loss 863.63 Time Consume 1.851\n",
      "Epoch 906/4999 Training Loss 863.64 Time Consume 1.865\n",
      "Epoch 907/4999 Training Loss 863.58 Time Consume 1.840\n",
      "Epoch 908/4999 Training Loss 863.62 Time Consume 1.830\n",
      "Epoch 909/4999 Training Loss 863.57 Time Consume 1.911\n",
      "Epoch 910/4999 Training Loss 863.58 Time Consume 1.951\n",
      "------------------------------------------------------------\n",
      "Evaluation 910/4999 Loss 856.84\n",
      "------------------------------------------------------------\n",
      "Epoch 911/4999 Training Loss 863.64 Time Consume 1.776\n",
      "Epoch 912/4999 Training Loss 863.58 Time Consume 1.690\n",
      "Epoch 913/4999 Training Loss 863.63 Time Consume 1.747\n",
      "Epoch 914/4999 Training Loss 863.59 Time Consume 1.878\n",
      "Epoch 915/4999 Training Loss 863.67 Time Consume 1.923\n",
      "Epoch 916/4999 Training Loss 863.60 Time Consume 1.868\n",
      "Epoch 917/4999 Training Loss 863.61 Time Consume 1.866\n",
      "Epoch 918/4999 Training Loss 863.62 Time Consume 1.845\n",
      "Epoch 919/4999 Training Loss 863.58 Time Consume 1.919\n",
      "Epoch 920/4999 Training Loss 863.53 Time Consume 1.886\n",
      "------------------------------------------------------------\n",
      "Evaluation 920/4999 Loss 856.74\n",
      "------------------------------------------------------------\n",
      "Epoch 921/4999 Training Loss 863.69 Time Consume 1.839\n",
      "Epoch 922/4999 Training Loss 863.65 Time Consume 1.896\n",
      "Epoch 923/4999 Training Loss 863.55 Time Consume 1.902\n",
      "Epoch 924/4999 Training Loss 863.65 Time Consume 2.017\n",
      "Epoch 925/4999 Training Loss 863.60 Time Consume 1.839\n",
      "Epoch 926/4999 Training Loss 863.71 Time Consume 1.866\n",
      "Epoch 927/4999 Training Loss 863.55 Time Consume 1.843\n",
      "Epoch 928/4999 Training Loss 863.63 Time Consume 1.830\n",
      "Epoch 929/4999 Training Loss 863.55 Time Consume 1.825\n",
      "Epoch 930/4999 Training Loss 863.53 Time Consume 1.812\n",
      "------------------------------------------------------------\n",
      "Evaluation 930/4999 Loss 856.68\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 931/4999 Training Loss 863.65 Time Consume 1.797\n",
      "Epoch 932/4999 Training Loss 863.61 Time Consume 1.917\n",
      "Epoch 933/4999 Training Loss 863.52 Time Consume 1.842\n",
      "Epoch 934/4999 Training Loss 863.53 Time Consume 1.831\n",
      "Epoch 935/4999 Training Loss 863.62 Time Consume 1.821\n",
      "Epoch 936/4999 Training Loss 863.54 Time Consume 1.826\n",
      "Epoch 937/4999 Training Loss 863.60 Time Consume 1.820\n",
      "Epoch 938/4999 Training Loss 863.62 Time Consume 1.830\n",
      "Epoch 939/4999 Training Loss 863.57 Time Consume 1.839\n",
      "Epoch 940/4999 Training Loss 863.61 Time Consume 1.829\n",
      "------------------------------------------------------------\n",
      "Evaluation 940/4999 Loss 856.80\n",
      "------------------------------------------------------------\n",
      "Epoch 941/4999 Training Loss 863.47 Time Consume 1.795\n",
      "Epoch 942/4999 Training Loss 863.53 Time Consume 1.793\n",
      "Epoch 943/4999 Training Loss 863.49 Time Consume 1.824\n",
      "Epoch 944/4999 Training Loss 863.47 Time Consume 1.831\n",
      "Epoch 945/4999 Training Loss 863.63 Time Consume 1.824\n",
      "Epoch 946/4999 Training Loss 863.54 Time Consume 1.839\n",
      "Epoch 947/4999 Training Loss 863.54 Time Consume 1.834\n",
      "Epoch 948/4999 Training Loss 863.60 Time Consume 1.890\n",
      "Epoch 949/4999 Training Loss 863.54 Time Consume 1.839\n",
      "Epoch 950/4999 Training Loss 863.52 Time Consume 1.854\n",
      "------------------------------------------------------------\n",
      "Evaluation 950/4999 Loss 856.69\n",
      "------------------------------------------------------------\n",
      "Epoch 951/4999 Training Loss 863.50 Time Consume 1.857\n",
      "Epoch 952/4999 Training Loss 863.58 Time Consume 1.819\n",
      "Epoch 953/4999 Training Loss 863.53 Time Consume 1.881\n",
      "Epoch 954/4999 Training Loss 863.58 Time Consume 1.811\n",
      "Epoch 955/4999 Training Loss 863.53 Time Consume 1.844\n",
      "Epoch 956/4999 Training Loss 863.47 Time Consume 1.874\n",
      "Epoch 957/4999 Training Loss 863.46 Time Consume 1.912\n",
      "Epoch 958/4999 Training Loss 863.55 Time Consume 1.828\n",
      "Epoch 959/4999 Training Loss 863.53 Time Consume 1.798\n",
      "Epoch 960/4999 Training Loss 863.61 Time Consume 1.723\n",
      "------------------------------------------------------------\n",
      "Evaluation 960/4999 Loss 856.68\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 961/4999 Training Loss 863.53 Time Consume 1.792\n",
      "Epoch 962/4999 Training Loss 863.54 Time Consume 1.834\n",
      "Epoch 963/4999 Training Loss 863.60 Time Consume 1.927\n",
      "Epoch 964/4999 Training Loss 863.53 Time Consume 1.945\n",
      "Epoch 965/4999 Training Loss 863.52 Time Consume 1.922\n",
      "Epoch 966/4999 Training Loss 863.49 Time Consume 1.697\n",
      "Epoch 967/4999 Training Loss 863.49 Time Consume 1.712\n",
      "Epoch 968/4999 Training Loss 863.50 Time Consume 1.704\n",
      "Epoch 969/4999 Training Loss 863.46 Time Consume 1.761\n",
      "Epoch 970/4999 Training Loss 863.52 Time Consume 1.827\n",
      "------------------------------------------------------------\n",
      "Evaluation 970/4999 Loss 856.63\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 971/4999 Training Loss 863.54 Time Consume 1.950\n",
      "Epoch 972/4999 Training Loss 863.56 Time Consume 1.826\n",
      "Epoch 973/4999 Training Loss 863.59 Time Consume 1.918\n",
      "Epoch 974/4999 Training Loss 863.59 Time Consume 1.958\n",
      "Epoch 975/4999 Training Loss 863.51 Time Consume 1.844\n",
      "Epoch 976/4999 Training Loss 863.45 Time Consume 1.860\n",
      "Epoch 977/4999 Training Loss 863.62 Time Consume 1.824\n",
      "Epoch 978/4999 Training Loss 863.52 Time Consume 1.839\n",
      "Epoch 979/4999 Training Loss 863.46 Time Consume 1.737\n",
      "Epoch 980/4999 Training Loss 863.53 Time Consume 1.851\n",
      "------------------------------------------------------------\n",
      "Evaluation 980/4999 Loss 856.66\n",
      "------------------------------------------------------------\n",
      "Epoch 981/4999 Training Loss 863.52 Time Consume 1.941\n",
      "Epoch 982/4999 Training Loss 863.52 Time Consume 1.899\n",
      "Epoch 983/4999 Training Loss 863.57 Time Consume 1.857\n",
      "Epoch 984/4999 Training Loss 863.43 Time Consume 1.866\n",
      "Epoch 985/4999 Training Loss 863.51 Time Consume 1.853\n",
      "Epoch 986/4999 Training Loss 863.47 Time Consume 1.839\n",
      "Epoch 987/4999 Training Loss 863.45 Time Consume 1.832\n",
      "Epoch 988/4999 Training Loss 863.56 Time Consume 1.839\n",
      "Epoch 989/4999 Training Loss 863.40 Time Consume 1.818\n",
      "Epoch 990/4999 Training Loss 863.51 Time Consume 1.936\n",
      "------------------------------------------------------------\n",
      "Evaluation 990/4999 Loss 856.58\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 991/4999 Training Loss 863.53 Time Consume 1.874\n",
      "Epoch 992/4999 Training Loss 863.47 Time Consume 1.997\n",
      "Epoch 993/4999 Training Loss 863.47 Time Consume 1.745\n",
      "Epoch 994/4999 Training Loss 863.49 Time Consume 1.679\n",
      "Epoch 995/4999 Training Loss 863.43 Time Consume 1.752\n",
      "Epoch 996/4999 Training Loss 863.44 Time Consume 1.842\n",
      "Epoch 997/4999 Training Loss 863.45 Time Consume 1.968\n",
      "Epoch 998/4999 Training Loss 863.58 Time Consume 1.850\n",
      "Epoch 999/4999 Training Loss 863.55 Time Consume 1.937\n",
      "Epoch 1000/4999 Training Loss 863.54 Time Consume 1.842\n",
      "------------------------------------------------------------\n",
      "Evaluation 1000/4999 Loss 856.60\n",
      "------------------------------------------------------------\n",
      "Epoch 1001/4999 Training Loss 863.46 Time Consume 1.860\n",
      "Epoch 1002/4999 Training Loss 863.39 Time Consume 1.865\n",
      "Epoch 1003/4999 Training Loss 863.42 Time Consume 1.805\n",
      "Epoch 1004/4999 Training Loss 863.42 Time Consume 1.787\n",
      "Epoch 1005/4999 Training Loss 863.41 Time Consume 2.014\n",
      "Epoch 1006/4999 Training Loss 863.46 Time Consume 1.826\n",
      "Epoch 1007/4999 Training Loss 863.42 Time Consume 1.821\n",
      "Epoch 1008/4999 Training Loss 863.46 Time Consume 1.859\n",
      "Epoch 1009/4999 Training Loss 863.44 Time Consume 1.699\n",
      "Epoch 1010/4999 Training Loss 863.43 Time Consume 1.886\n",
      "------------------------------------------------------------\n",
      "Evaluation 1010/4999 Loss 856.63\n",
      "------------------------------------------------------------\n",
      "Epoch 1011/4999 Training Loss 863.44 Time Consume 1.984\n",
      "Epoch 1012/4999 Training Loss 863.49 Time Consume 1.777\n",
      "Epoch 1013/4999 Training Loss 863.38 Time Consume 1.863\n",
      "Epoch 1014/4999 Training Loss 863.45 Time Consume 1.863\n",
      "Epoch 1015/4999 Training Loss 863.48 Time Consume 1.867\n",
      "Epoch 1016/4999 Training Loss 863.44 Time Consume 1.826\n",
      "Epoch 1017/4999 Training Loss 863.46 Time Consume 1.825\n",
      "Epoch 1018/4999 Training Loss 863.41 Time Consume 1.831\n",
      "Epoch 1019/4999 Training Loss 863.43 Time Consume 1.848\n",
      "Epoch 1020/4999 Training Loss 863.46 Time Consume 1.898\n",
      "------------------------------------------------------------\n",
      "Evaluation 1020/4999 Loss 856.55\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1021/4999 Training Loss 863.45 Time Consume 1.841\n",
      "Epoch 1022/4999 Training Loss 863.46 Time Consume 1.832\n",
      "Epoch 1023/4999 Training Loss 863.40 Time Consume 1.916\n",
      "Epoch 1024/4999 Training Loss 863.45 Time Consume 1.815\n",
      "Epoch 1025/4999 Training Loss 863.41 Time Consume 1.852\n",
      "Epoch 1026/4999 Training Loss 863.41 Time Consume 1.861\n",
      "Epoch 1027/4999 Training Loss 863.41 Time Consume 1.912\n",
      "Epoch 1028/4999 Training Loss 863.37 Time Consume 1.884\n",
      "Epoch 1029/4999 Training Loss 863.40 Time Consume 1.883\n",
      "Epoch 1030/4999 Training Loss 863.47 Time Consume 1.823\n",
      "------------------------------------------------------------\n",
      "Evaluation 1030/4999 Loss 856.66\n",
      "------------------------------------------------------------\n",
      "Epoch 1031/4999 Training Loss 863.41 Time Consume 1.845\n",
      "Epoch 1032/4999 Training Loss 863.44 Time Consume 1.822\n",
      "Epoch 1033/4999 Training Loss 863.40 Time Consume 1.825\n",
      "Epoch 1034/4999 Training Loss 863.40 Time Consume 1.855\n",
      "Epoch 1035/4999 Training Loss 863.44 Time Consume 1.791\n",
      "Epoch 1036/4999 Training Loss 863.41 Time Consume 1.885\n",
      "Epoch 1037/4999 Training Loss 863.38 Time Consume 1.841\n",
      "Epoch 1038/4999 Training Loss 863.42 Time Consume 1.848\n",
      "Epoch 1039/4999 Training Loss 863.41 Time Consume 1.960\n",
      "Epoch 1040/4999 Training Loss 863.43 Time Consume 2.092\n",
      "------------------------------------------------------------\n",
      "Evaluation 1040/4999 Loss 856.59\n",
      "------------------------------------------------------------\n",
      "Epoch 1041/4999 Training Loss 863.42 Time Consume 1.842\n",
      "Epoch 1042/4999 Training Loss 863.37 Time Consume 1.867\n",
      "Epoch 1043/4999 Training Loss 863.41 Time Consume 1.842\n",
      "Epoch 1044/4999 Training Loss 863.38 Time Consume 1.869\n",
      "Epoch 1045/4999 Training Loss 863.43 Time Consume 1.807\n",
      "Epoch 1046/4999 Training Loss 863.34 Time Consume 1.836\n",
      "Epoch 1047/4999 Training Loss 863.42 Time Consume 1.841\n",
      "Epoch 1048/4999 Training Loss 863.45 Time Consume 1.839\n",
      "Epoch 1049/4999 Training Loss 863.50 Time Consume 1.846\n",
      "Epoch 1050/4999 Training Loss 863.45 Time Consume 1.842\n",
      "------------------------------------------------------------\n",
      "Evaluation 1050/4999 Loss 856.52\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1051/4999 Training Loss 863.35 Time Consume 1.828\n",
      "Epoch 1052/4999 Training Loss 863.39 Time Consume 1.788\n",
      "Epoch 1053/4999 Training Loss 863.35 Time Consume 1.781\n",
      "Epoch 1054/4999 Training Loss 863.37 Time Consume 1.836\n",
      "Epoch 1055/4999 Training Loss 863.38 Time Consume 1.853\n",
      "Epoch 1056/4999 Training Loss 863.43 Time Consume 1.866\n",
      "Epoch 1057/4999 Training Loss 863.42 Time Consume 1.830\n",
      "Epoch 1058/4999 Training Loss 863.40 Time Consume 1.836\n",
      "Epoch 1059/4999 Training Loss 863.39 Time Consume 1.836\n",
      "Epoch 1060/4999 Training Loss 863.44 Time Consume 1.838\n",
      "------------------------------------------------------------\n",
      "Evaluation 1060/4999 Loss 856.54\n",
      "------------------------------------------------------------\n",
      "Epoch 1061/4999 Training Loss 863.36 Time Consume 1.834\n",
      "Epoch 1062/4999 Training Loss 863.37 Time Consume 1.820\n",
      "Epoch 1063/4999 Training Loss 863.31 Time Consume 1.824\n",
      "Epoch 1064/4999 Training Loss 863.34 Time Consume 1.826\n",
      "Epoch 1065/4999 Training Loss 863.40 Time Consume 1.947\n",
      "Epoch 1066/4999 Training Loss 863.33 Time Consume 1.861\n",
      "Epoch 1067/4999 Training Loss 863.45 Time Consume 1.927\n",
      "Epoch 1068/4999 Training Loss 863.38 Time Consume 1.741\n",
      "Epoch 1069/4999 Training Loss 863.41 Time Consume 1.966\n",
      "Epoch 1070/4999 Training Loss 863.38 Time Consume 1.924\n",
      "------------------------------------------------------------\n",
      "Evaluation 1070/4999 Loss 856.57\n",
      "------------------------------------------------------------\n",
      "Epoch 1071/4999 Training Loss 863.34 Time Consume 1.821\n",
      "Epoch 1072/4999 Training Loss 863.28 Time Consume 1.846\n",
      "Epoch 1073/4999 Training Loss 863.27 Time Consume 1.843\n",
      "Epoch 1074/4999 Training Loss 863.41 Time Consume 1.867\n",
      "Epoch 1075/4999 Training Loss 863.36 Time Consume 1.828\n",
      "Epoch 1076/4999 Training Loss 863.38 Time Consume 1.833\n",
      "Epoch 1077/4999 Training Loss 863.42 Time Consume 1.828\n",
      "Epoch 1078/4999 Training Loss 863.46 Time Consume 1.821\n",
      "Epoch 1079/4999 Training Loss 863.35 Time Consume 1.823\n",
      "Epoch 1080/4999 Training Loss 863.31 Time Consume 1.809\n",
      "------------------------------------------------------------\n",
      "Evaluation 1080/4999 Loss 856.53\n",
      "------------------------------------------------------------\n",
      "Epoch 1081/4999 Training Loss 863.41 Time Consume 1.830\n",
      "Epoch 1082/4999 Training Loss 863.38 Time Consume 1.839\n",
      "Epoch 1083/4999 Training Loss 863.33 Time Consume 1.881\n",
      "Epoch 1084/4999 Training Loss 863.39 Time Consume 1.825\n",
      "Epoch 1085/4999 Training Loss 863.36 Time Consume 1.871\n",
      "Epoch 1086/4999 Training Loss 863.31 Time Consume 1.853\n",
      "Epoch 1087/4999 Training Loss 863.31 Time Consume 1.839\n",
      "Epoch 1088/4999 Training Loss 863.32 Time Consume 1.821\n",
      "Epoch 1089/4999 Training Loss 863.35 Time Consume 1.733\n",
      "Epoch 1090/4999 Training Loss 863.50 Time Consume 1.805\n",
      "------------------------------------------------------------\n",
      "Evaluation 1090/4999 Loss 856.50\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1091/4999 Training Loss 863.28 Time Consume 1.852\n",
      "Epoch 1092/4999 Training Loss 863.38 Time Consume 1.837\n",
      "Epoch 1093/4999 Training Loss 863.35 Time Consume 1.852\n",
      "Epoch 1094/4999 Training Loss 863.37 Time Consume 1.838\n",
      "Epoch 1095/4999 Training Loss 863.40 Time Consume 1.849\n",
      "Epoch 1096/4999 Training Loss 863.33 Time Consume 1.827\n",
      "Epoch 1097/4999 Training Loss 863.36 Time Consume 1.822\n",
      "Epoch 1098/4999 Training Loss 863.36 Time Consume 1.840\n",
      "Epoch 1099/4999 Training Loss 863.25 Time Consume 1.904\n",
      "Epoch 1100/4999 Training Loss 863.28 Time Consume 1.865\n",
      "------------------------------------------------------------\n",
      "Evaluation 1100/4999 Loss 856.45\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1101/4999 Training Loss 863.32 Time Consume 1.842\n",
      "Epoch 1102/4999 Training Loss 863.34 Time Consume 1.930\n",
      "Epoch 1103/4999 Training Loss 863.29 Time Consume 1.834\n",
      "Epoch 1104/4999 Training Loss 863.33 Time Consume 1.771\n",
      "Epoch 1105/4999 Training Loss 863.40 Time Consume 1.801\n",
      "Epoch 1106/4999 Training Loss 863.40 Time Consume 1.835\n",
      "Epoch 1107/4999 Training Loss 863.29 Time Consume 1.830\n",
      "Epoch 1108/4999 Training Loss 863.33 Time Consume 1.790\n",
      "Epoch 1109/4999 Training Loss 863.26 Time Consume 1.854\n",
      "Epoch 1110/4999 Training Loss 863.35 Time Consume 1.766\n",
      "------------------------------------------------------------\n",
      "Evaluation 1110/4999 Loss 856.57\n",
      "------------------------------------------------------------\n",
      "Epoch 1111/4999 Training Loss 863.35 Time Consume 1.842\n",
      "Epoch 1112/4999 Training Loss 863.27 Time Consume 1.783\n",
      "Epoch 1113/4999 Training Loss 863.34 Time Consume 1.878\n",
      "Epoch 1114/4999 Training Loss 863.33 Time Consume 1.885\n",
      "Epoch 1115/4999 Training Loss 863.30 Time Consume 1.873\n",
      "Epoch 1116/4999 Training Loss 863.29 Time Consume 1.783\n",
      "Epoch 1117/4999 Training Loss 863.37 Time Consume 1.677\n",
      "Epoch 1118/4999 Training Loss 863.36 Time Consume 1.682\n",
      "Epoch 1119/4999 Training Loss 863.32 Time Consume 1.818\n",
      "Epoch 1120/4999 Training Loss 863.26 Time Consume 1.828\n",
      "------------------------------------------------------------\n",
      "Evaluation 1120/4999 Loss 856.49\n",
      "------------------------------------------------------------\n",
      "Epoch 1121/4999 Training Loss 863.39 Time Consume 1.838\n",
      "Epoch 1122/4999 Training Loss 863.31 Time Consume 1.838\n",
      "Epoch 1123/4999 Training Loss 863.29 Time Consume 1.891\n",
      "Epoch 1124/4999 Training Loss 863.28 Time Consume 1.972\n",
      "Epoch 1125/4999 Training Loss 863.29 Time Consume 1.865\n",
      "Epoch 1126/4999 Training Loss 863.30 Time Consume 1.819\n",
      "Epoch 1127/4999 Training Loss 863.35 Time Consume 1.840\n",
      "Epoch 1128/4999 Training Loss 863.28 Time Consume 1.850\n",
      "Epoch 1129/4999 Training Loss 863.33 Time Consume 1.929\n",
      "Epoch 1130/4999 Training Loss 863.30 Time Consume 1.851\n",
      "------------------------------------------------------------\n",
      "Evaluation 1130/4999 Loss 856.38\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1131/4999 Training Loss 863.29 Time Consume 1.880\n",
      "Epoch 1132/4999 Training Loss 863.33 Time Consume 1.779\n",
      "Epoch 1133/4999 Training Loss 863.26 Time Consume 1.839\n",
      "Epoch 1134/4999 Training Loss 863.31 Time Consume 1.911\n",
      "Epoch 1135/4999 Training Loss 863.27 Time Consume 1.841\n",
      "Epoch 1136/4999 Training Loss 863.23 Time Consume 1.900\n",
      "Epoch 1137/4999 Training Loss 863.30 Time Consume 1.902\n",
      "Epoch 1138/4999 Training Loss 863.31 Time Consume 1.712\n",
      "Epoch 1139/4999 Training Loss 863.33 Time Consume 1.714\n",
      "Epoch 1140/4999 Training Loss 863.32 Time Consume 1.788\n",
      "------------------------------------------------------------\n",
      "Evaluation 1140/4999 Loss 856.48\n",
      "------------------------------------------------------------\n",
      "Epoch 1141/4999 Training Loss 863.33 Time Consume 1.800\n",
      "Epoch 1142/4999 Training Loss 863.33 Time Consume 1.869\n",
      "Epoch 1143/4999 Training Loss 863.43 Time Consume 1.859\n",
      "Epoch 1144/4999 Training Loss 863.29 Time Consume 1.709\n",
      "Epoch 1145/4999 Training Loss 863.25 Time Consume 1.752\n",
      "Epoch 1146/4999 Training Loss 863.33 Time Consume 1.864\n",
      "Epoch 1147/4999 Training Loss 863.20 Time Consume 1.867\n",
      "Epoch 1148/4999 Training Loss 863.25 Time Consume 1.882\n",
      "Epoch 1149/4999 Training Loss 863.20 Time Consume 1.841\n",
      "Epoch 1150/4999 Training Loss 863.32 Time Consume 1.840\n",
      "------------------------------------------------------------\n",
      "Evaluation 1150/4999 Loss 856.41\n",
      "------------------------------------------------------------\n",
      "Epoch 1151/4999 Training Loss 863.28 Time Consume 1.842\n",
      "Epoch 1152/4999 Training Loss 863.30 Time Consume 1.825\n",
      "Epoch 1153/4999 Training Loss 863.29 Time Consume 1.983\n",
      "Epoch 1154/4999 Training Loss 863.29 Time Consume 1.895\n",
      "Epoch 1155/4999 Training Loss 863.22 Time Consume 1.824\n",
      "Epoch 1156/4999 Training Loss 863.28 Time Consume 1.956\n",
      "Epoch 1157/4999 Training Loss 863.27 Time Consume 1.929\n",
      "Epoch 1158/4999 Training Loss 863.28 Time Consume 1.827\n",
      "Epoch 1159/4999 Training Loss 863.25 Time Consume 1.887\n",
      "Epoch 1160/4999 Training Loss 863.26 Time Consume 1.878\n",
      "------------------------------------------------------------\n",
      "Evaluation 1160/4999 Loss 856.53\n",
      "------------------------------------------------------------\n",
      "Epoch 1161/4999 Training Loss 863.27 Time Consume 1.934\n",
      "Epoch 1162/4999 Training Loss 863.27 Time Consume 1.772\n",
      "Epoch 1163/4999 Training Loss 863.18 Time Consume 1.749\n",
      "Epoch 1164/4999 Training Loss 863.25 Time Consume 1.864\n",
      "Epoch 1165/4999 Training Loss 863.31 Time Consume 1.940\n",
      "Epoch 1166/4999 Training Loss 863.20 Time Consume 1.823\n",
      "Epoch 1167/4999 Training Loss 863.25 Time Consume 1.881\n",
      "Epoch 1168/4999 Training Loss 863.23 Time Consume 1.832\n",
      "Epoch 1169/4999 Training Loss 863.21 Time Consume 1.910\n",
      "Epoch 1170/4999 Training Loss 863.24 Time Consume 1.890\n",
      "------------------------------------------------------------\n",
      "Evaluation 1170/4999 Loss 856.36\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1171/4999 Training Loss 863.22 Time Consume 2.021\n",
      "Epoch 1172/4999 Training Loss 863.27 Time Consume 1.912\n",
      "Epoch 1173/4999 Training Loss 863.34 Time Consume 1.888\n",
      "Epoch 1174/4999 Training Loss 863.28 Time Consume 1.853\n",
      "Epoch 1175/4999 Training Loss 863.18 Time Consume 1.991\n",
      "Epoch 1176/4999 Training Loss 863.25 Time Consume 1.918\n",
      "Epoch 1177/4999 Training Loss 863.27 Time Consume 1.843\n",
      "Epoch 1178/4999 Training Loss 863.27 Time Consume 1.823\n",
      "Epoch 1179/4999 Training Loss 863.23 Time Consume 1.715\n",
      "Epoch 1180/4999 Training Loss 863.23 Time Consume 1.730\n",
      "------------------------------------------------------------\n",
      "Evaluation 1180/4999 Loss 856.42\n",
      "------------------------------------------------------------\n",
      "Epoch 1181/4999 Training Loss 863.19 Time Consume 1.773\n",
      "Epoch 1182/4999 Training Loss 863.19 Time Consume 1.855\n",
      "Epoch 1183/4999 Training Loss 863.24 Time Consume 1.847\n",
      "Epoch 1184/4999 Training Loss 863.18 Time Consume 1.764\n",
      "Epoch 1185/4999 Training Loss 863.21 Time Consume 1.752\n",
      "Epoch 1186/4999 Training Loss 863.22 Time Consume 1.879\n",
      "Epoch 1187/4999 Training Loss 863.23 Time Consume 1.860\n",
      "Epoch 1188/4999 Training Loss 863.22 Time Consume 1.822\n",
      "Epoch 1189/4999 Training Loss 863.23 Time Consume 1.768\n",
      "Epoch 1190/4999 Training Loss 863.29 Time Consume 1.860\n",
      "------------------------------------------------------------\n",
      "Evaluation 1190/4999 Loss 856.35\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1191/4999 Training Loss 863.21 Time Consume 1.880\n",
      "Epoch 1192/4999 Training Loss 863.19 Time Consume 1.882\n",
      "Epoch 1193/4999 Training Loss 863.19 Time Consume 1.744\n",
      "Epoch 1194/4999 Training Loss 863.21 Time Consume 1.736\n",
      "Epoch 1195/4999 Training Loss 863.19 Time Consume 1.833\n",
      "Epoch 1196/4999 Training Loss 863.27 Time Consume 1.883\n",
      "Epoch 1197/4999 Training Loss 863.28 Time Consume 1.890\n",
      "Epoch 1198/4999 Training Loss 863.26 Time Consume 1.791\n",
      "Epoch 1199/4999 Training Loss 863.18 Time Consume 2.037\n",
      "Epoch 1200/4999 Training Loss 863.20 Time Consume 1.820\n",
      "------------------------------------------------------------\n",
      "Evaluation 1200/4999 Loss 856.36\n",
      "------------------------------------------------------------\n",
      "Epoch 1201/4999 Training Loss 863.21 Time Consume 1.839\n",
      "Epoch 1202/4999 Training Loss 863.22 Time Consume 1.995\n",
      "Epoch 1203/4999 Training Loss 863.22 Time Consume 1.854\n",
      "Epoch 1204/4999 Training Loss 863.32 Time Consume 1.923\n",
      "Epoch 1205/4999 Training Loss 863.22 Time Consume 1.916\n",
      "Epoch 1206/4999 Training Loss 863.24 Time Consume 1.927\n",
      "Epoch 1207/4999 Training Loss 863.22 Time Consume 1.878\n",
      "Epoch 1208/4999 Training Loss 863.25 Time Consume 1.940\n",
      "Epoch 1209/4999 Training Loss 863.21 Time Consume 1.869\n",
      "Epoch 1210/4999 Training Loss 863.19 Time Consume 1.946\n",
      "------------------------------------------------------------\n",
      "Evaluation 1210/4999 Loss 856.36\n",
      "------------------------------------------------------------\n",
      "Epoch 1211/4999 Training Loss 863.21 Time Consume 1.794\n",
      "Epoch 1212/4999 Training Loss 863.22 Time Consume 1.742\n",
      "Epoch 1213/4999 Training Loss 863.22 Time Consume 1.798\n",
      "Epoch 1214/4999 Training Loss 863.22 Time Consume 1.937\n",
      "Epoch 1215/4999 Training Loss 863.22 Time Consume 1.934\n",
      "Epoch 1216/4999 Training Loss 863.20 Time Consume 1.931\n",
      "Epoch 1217/4999 Training Loss 863.18 Time Consume 2.077\n",
      "Epoch 1218/4999 Training Loss 863.26 Time Consume 1.891\n",
      "Epoch 1219/4999 Training Loss 863.20 Time Consume 2.004\n",
      "Epoch 1220/4999 Training Loss 863.19 Time Consume 1.869\n",
      "------------------------------------------------------------\n",
      "Evaluation 1220/4999 Loss 856.45\n",
      "------------------------------------------------------------\n",
      "Epoch 1221/4999 Training Loss 863.25 Time Consume 1.809\n",
      "Epoch 1222/4999 Training Loss 863.17 Time Consume 1.853\n",
      "Epoch 1223/4999 Training Loss 863.16 Time Consume 1.858\n",
      "Epoch 1224/4999 Training Loss 863.14 Time Consume 1.907\n",
      "Epoch 1225/4999 Training Loss 863.20 Time Consume 1.851\n",
      "Epoch 1226/4999 Training Loss 863.19 Time Consume 1.869\n",
      "Epoch 1227/4999 Training Loss 863.19 Time Consume 1.793\n",
      "Epoch 1228/4999 Training Loss 863.34 Time Consume 1.845\n",
      "Epoch 1229/4999 Training Loss 863.15 Time Consume 1.832\n",
      "Epoch 1230/4999 Training Loss 863.17 Time Consume 1.864\n",
      "------------------------------------------------------------\n",
      "Evaluation 1230/4999 Loss 856.33\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1231/4999 Training Loss 863.15 Time Consume 1.856\n",
      "Epoch 1232/4999 Training Loss 863.18 Time Consume 1.847\n",
      "Epoch 1233/4999 Training Loss 863.22 Time Consume 1.912\n",
      "Epoch 1234/4999 Training Loss 863.20 Time Consume 1.954\n",
      "Epoch 1235/4999 Training Loss 863.16 Time Consume 1.904\n",
      "Epoch 1236/4999 Training Loss 863.13 Time Consume 1.771\n",
      "Epoch 1237/4999 Training Loss 863.20 Time Consume 1.871\n",
      "Epoch 1238/4999 Training Loss 863.17 Time Consume 1.943\n",
      "Epoch 1239/4999 Training Loss 863.17 Time Consume 1.872\n",
      "Epoch 1240/4999 Training Loss 863.17 Time Consume 1.863\n",
      "------------------------------------------------------------\n",
      "Evaluation 1240/4999 Loss 856.37\n",
      "------------------------------------------------------------\n",
      "Epoch 1241/4999 Training Loss 863.19 Time Consume 1.952\n",
      "Epoch 1242/4999 Training Loss 863.15 Time Consume 1.866\n",
      "Epoch 1243/4999 Training Loss 863.17 Time Consume 1.887\n",
      "Epoch 1244/4999 Training Loss 863.10 Time Consume 1.829\n",
      "Epoch 1245/4999 Training Loss 863.15 Time Consume 1.801\n",
      "Epoch 1246/4999 Training Loss 863.18 Time Consume 1.916\n",
      "Epoch 1247/4999 Training Loss 863.32 Time Consume 1.951\n",
      "Epoch 1248/4999 Training Loss 863.11 Time Consume 1.777\n",
      "Epoch 1249/4999 Training Loss 863.15 Time Consume 1.740\n",
      "Epoch 1250/4999 Training Loss 863.16 Time Consume 1.720\n",
      "------------------------------------------------------------\n",
      "Evaluation 1250/4999 Loss 856.38\n",
      "------------------------------------------------------------\n",
      "Epoch 1251/4999 Training Loss 863.17 Time Consume 1.717\n",
      "Epoch 1252/4999 Training Loss 863.18 Time Consume 1.849\n",
      "Epoch 1253/4999 Training Loss 863.10 Time Consume 1.832\n",
      "Epoch 1254/4999 Training Loss 863.23 Time Consume 1.862\n",
      "Epoch 1255/4999 Training Loss 863.19 Time Consume 1.886\n",
      "Epoch 1256/4999 Training Loss 863.27 Time Consume 1.942\n",
      "Epoch 1257/4999 Training Loss 863.19 Time Consume 1.852\n",
      "Epoch 1258/4999 Training Loss 863.15 Time Consume 1.975\n",
      "Epoch 1259/4999 Training Loss 863.11 Time Consume 1.837\n",
      "Epoch 1260/4999 Training Loss 863.23 Time Consume 2.013\n",
      "------------------------------------------------------------\n",
      "Evaluation 1260/4999 Loss 856.31\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1261/4999 Training Loss 863.17 Time Consume 1.908\n",
      "Epoch 1262/4999 Training Loss 863.17 Time Consume 1.910\n",
      "Epoch 1263/4999 Training Loss 863.17 Time Consume 1.924\n",
      "Epoch 1264/4999 Training Loss 863.13 Time Consume 1.794\n",
      "Epoch 1265/4999 Training Loss 863.16 Time Consume 1.733\n",
      "Epoch 1266/4999 Training Loss 863.21 Time Consume 1.792\n",
      "Epoch 1267/4999 Training Loss 863.13 Time Consume 1.942\n",
      "Epoch 1268/4999 Training Loss 863.14 Time Consume 1.856\n",
      "Epoch 1269/4999 Training Loss 863.19 Time Consume 1.849\n",
      "Epoch 1270/4999 Training Loss 863.14 Time Consume 1.837\n",
      "------------------------------------------------------------\n",
      "Evaluation 1270/4999 Loss 856.31\n",
      "Update Model\n",
      "------------------------------------------------------------\n",
      "Epoch 1271/4999 Training Loss 863.17 Time Consume 1.786\n",
      "Epoch 1272/4999 Training Loss 863.12 Time Consume 1.910\n",
      "Epoch 1273/4999 Training Loss 863.14 Time Consume 1.916\n",
      "Epoch 1274/4999 Training Loss 863.16 Time Consume 1.981\n",
      "Epoch 1275/4999 Training Loss 863.24 Time Consume 1.845\n",
      "Epoch 1276/4999 Training Loss 863.27 Time Consume 1.891\n",
      "Epoch 1277/4999 Training Loss 863.08 Time Consume 1.859\n",
      "Epoch 1278/4999 Training Loss 863.27 Time Consume 2.047\n",
      "Epoch 1279/4999 Training Loss 863.13 Time Consume 1.938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zhuzhoufan/EMPIRICAL-ASSET-PRICING-VIA-THE-CONDITIONAL-QUANTILE-VARIATIONAL-AUTOENCODER/test.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.2.58.249/home/zhuzhoufan/EMPIRICAL-ASSET-PRICING-VIA-THE-CONDITIONAL-QUANTILE-VARIATIONAL-AUTOENCODER/test.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m AE_factor\u001b[39m.\u001b[39;49mtrain(AE_factor_epoch_num, lam)\n",
      "File \u001b[0;32m~/EMPIRICAL-ASSET-PRICING-VIA-THE-CONDITIONAL-QUANTILE-VARIATIONAL-AUTOENCODER/AE/AE_Factor.py:95\u001b[0m, in \u001b[0;36mAE_Factor_Agent.train\u001b[0;34m(self, epoch_num, lam)\u001b[0m\n\u001b[1;32m     93\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(y_hat, y) \u001b[39m+\u001b[39m lam \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m L1_loss\n\u001b[1;32m     94\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_params(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, loss, networks\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mfactor_loading_network])\n\u001b[1;32m     96\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     97\u001b[0m dtime \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/EMPIRICAL-ASSET-PRICING-VIA-THE-CONDITIONAL-QUANTILE-VARIATIONAL-AUTOENCODER/AE/AE_Factor.py:45\u001b[0m, in \u001b[0;36mAE_Factor_Agent.update_params\u001b[0;34m(self, optim, loss, networks, retain_graph, grad_cliping)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mBack propagation\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     44\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph \u001b[39m=\u001b[39;49m retain_graph)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m grad_cliping:\n\u001b[1;32m     47\u001b[0m     \u001b[39mfor\u001b[39;00m net \u001b[39min\u001b[39;00m networks:\n",
      "File \u001b[0;32m~/miniconda3/envs/QAE/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/QAE/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AE_factor.train(AE_factor_epoch_num, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_factor.network.load_state_dict(torch.load(f'{log_dir}/AEF_best.pth'))\n",
    "true_r = AE_factor.label_test.numpy()\n",
    "r_hat_total = AE_factor.network.forward(AE_factor.feature_test.to(AE_factor.network.device)).cpu().detach().numpy()\n",
    "R_total = 1 - np.sum(np.power(true_r - r_hat_total, 2)) / np.sum(np.power(true_r, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4552391767501831"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([133, 1000, 50])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = AE_factor.feature_test\n",
    "r_test = feature_test[:, :, -1]\n",
    "c_test = feature_test[:, :, :-1]\n",
    "\n",
    "portfolio_test = torch.zeros((r_test.shape[0], P), device = AE_factor.network.device)\n",
    "for t in range(r_test.shape[0]):\n",
    "    portfolio_test[t, :] = torch.inverse(c_test[t, :, :].t() @ c_test[t, :, :]) @ c_test[t, :, :].t() @ r_test[t, :]\n",
    "\n",
    "latent_ = AE_factor.network.Encoder(portfolio_test.to(AE_factor.network.device))\n",
    "moving_latent = torch.zeros_like(latent_)\n",
    "for j in range(bandwidth, latent_.shape[0], 1):\n",
    "    moving_latent[j, :] = latent_[(j - bandwidth):j, :].mean(axis = 0)\n",
    "beta = AE_factor.network.factor_loading_network(c_test.to(AE_factor.network.device))\n",
    "r_hat_pred = torch.bmm(beta[bandwidth:, :, :], torch.unsqueeze(moving_latent[bandwidth:, :], axis = 2)).cpu().detach().numpy()\n",
    "R_pred = 1 - np.sum(np.power(true_r[bandwidth:, :, :] - r_hat_pred, 2)) / np.sum(np.power(true_r[bandwidth:, :, :], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04230237007141113"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tab = pd.DataFrame(columns = ['R_total', 'R_pred'])\n",
    "for repetition_index in range(repetition_num):\n",
    "    R_total, R_pred = train_once(repetition_index, bandwidth, N, T, P_f, P_x, P_c, W, linear_index, K, lr, f_hidden_dim, lam, AE_epoch_num, AE_factor_epoch_num, root_log_dir, seed)\n",
    "    result_tab.loc[repetition_index, 'R_total'] = R_total\n",
    "    result_tab.loc[repetition_index, 'R_pred'] = R_pred\n",
    "    result_tab.to_csv(f'{root_log_dir}/result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('QAE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9a7173eeb8052d1458e8817087213c5ec04757add6840991b87bde09da78a31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
